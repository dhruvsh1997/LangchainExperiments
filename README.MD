# ğŸ§  LangChain Experiments & Tutorials

![LangChain](https://img.shields.io/badge/LangChain-Framework-green)
![Notebooks](https://img.shields.io/badge/Format-Colab_Notebooks-blue)
![License](https://img.shields.io/badge/License-MIT-yellow)
![Python](https://img.shields.io/badge/Python-3.8+-blue)

A comprehensive collection of **hands-on Colab-style notebooks** for practical implementations and benchmarking of core LangChain components. Master the entire LLM application stackâ€”from document processing to embeddings, vector stores, retrievers, and complete RAG pipelines through structured, modular learning.

## ğŸ¯ What You'll Learn

This repository provides a systematic journey through the LangChain ecosystem, with each component carefully isolated to enable:

- **ğŸ“ Focused Learning** - Master individual components without complexity
- **âš¡ Performance Benchmarking** - Compare implementations with real metrics
- **ğŸ”§ Production-Ready Solutions** - Plug-and-play components for your applications
- **ğŸ“Š Visual Analytics** - Interactive metrics and performance comparisons

## ğŸ“ Repository Structure

```
LangChain-Playlist/
â”œâ”€â”€ ğŸ“„ Loader and Splitters/
â”‚   â”œâ”€â”€ PDF Based/
â”‚   â”‚   â”œâ”€â”€ PyPDFLoader_with_SemanticChunker.ipynb
â”‚   â”‚   â”œâ”€â”€ UnstructuredPDFLoader_with_RecursiveCharacterTextSplitter.ipynb
â”‚   â”‚   â””â”€â”€ PyMuPDFLoaderWithRecursiveCharacterTextSplitter.ipynb
â”‚   â”œâ”€â”€ TextFile Based/
â”‚   â”‚   â””â”€â”€ TextLoader_with_SemanticChunker.ipynb
â”‚   â”œâ”€â”€ ExcelFile Based/
â”‚   â”‚   â””â”€â”€ UnstructuredExcelLoader_with_CharacterTextSplitter.ipynb
â”‚   â”œâ”€â”€ Document Based/
â”‚   â”‚   â””â”€â”€ UnstructuredWordDocumentLoader_with_RecursiveCharacterTextSplitter.ipynb
â”‚   â”œâ”€â”€ PPT Based/
â”‚   â”‚   â””â”€â”€ UnstructuredPowerPointLoader_with_CharacterTextSplitter.ipynb
â”‚   â”œâ”€â”€ JSON Based/
â”‚   â”‚   â””â”€â”€ JSONLoader_with_RecursiveJsonSplitter.ipynb
â”‚   â”œâ”€â”€ HTML_URL Based/
â”‚   â”‚   â”œâ”€â”€ BSHTMLLoaderWithHTMLHeaderTextSplitter.ipynb
â”‚   â”‚   â””â”€â”€ UnstructuredHTMLLoaderWithRecursiveCharacterTextSplitter.ipynb
â”‚   â””â”€â”€ Markdown_Based/
â”‚       â””â”€â”€ UnstructuredMarkdownLoader_with_MarkdownHeaderTextSplitter.ipynb
â”‚
â”œâ”€â”€ ğŸ§¬ Embedding Models/
â”‚   â”œâ”€â”€ BASE Embedding/
â”‚   â”‚   â”œâ”€â”€ all-MiniLM-L12-v2.ipynb
â”‚   â”‚   â”œâ”€â”€ all-MiniLM-L6-v2.ipynb
â”‚   â”‚   â”œâ”€â”€ gte-multilingual-base.ipynb
â”‚   â”‚   â”œâ”€â”€ OpenAIEmbedding-text_embedding_3_large.ipynb
â”‚   â”‚   â”œâ”€â”€ GIST-Embedding-v0.ipynb
â”‚   â”‚   â”œâ”€â”€ cohere_embed_english_v3_0.ipynb
â”‚   â”‚   â”œâ”€â”€ all-mpnet-base-v2.ipynb
â”‚   â”‚   â”œâ”€â”€ multi-qa-mpnet-base-dot-v1.ipynb
â”‚   â”‚   â””â”€â”€ msmarco_distilbert_cos_v5.ipynb
â”‚   â””â”€â”€ RERANKER Models/
â”‚       â”œâ”€â”€ ms-marco-MiniLM-L-6-v2.ipynb
â”‚       â”œâ”€â”€ ms-marco-MiniLM-L-12-v2.ipynb
â”‚       â”œâ”€â”€ tomaarsen-ModernBERT-base-gooaq-bce.ipynb
â”‚       â”œâ”€â”€ quora-distilroberta-base.ipynb
â”‚       â”œâ”€â”€ stsb-roberta-large.ipynb
â”‚       â””â”€â”€ qnli-electra-base.ipynb
â”‚
â”œâ”€â”€ ğŸ§² VectorDB's/
â”‚   â”œâ”€â”€ FAISS.ipynb
â”‚   â”œâ”€â”€ ChromaDB.ipynb
â”‚   â”œâ”€â”€ Weaviate.ipynb
â”‚   â”œâ”€â”€ Qdrant.ipynb
â”‚   â”œâ”€â”€ Pinecone.ipynb
â”‚   â””â”€â”€ Redis.ipynb
â”‚
â”œâ”€â”€ ğŸ“ PromptsTemplates/
â”‚   â”œâ”€â”€ PromptTemplateExample.ipynb
â”‚   â”œâ”€â”€ ChatPromptTemplateExample.ipynb
â”‚   â”œâ”€â”€ ChatMessagePromptTemplateExample.ipynb
â”‚   â”œâ”€â”€ ImagePromptTemplateExample.ipynb
â”‚   â””â”€â”€ PipelinePromptTemplateExample.ipynb
â”‚
â”œâ”€â”€ ğŸ”§ Output Parsers/
â”‚   â”œâ”€â”€ OutputParser_StrOutputParser.ipynb
â”‚   â”œâ”€â”€ OutputParser_JSONOutputParser.ipynb
â”‚   â””â”€â”€ OutputParser_XMLOutputParser.ipynb
â”‚
â”œâ”€â”€ ğŸ” Retrievers/
â”‚   â”œâ”€â”€ VectorStoreRetrieverMMR-InsideRAG.ipynb
â”‚   â”œâ”€â”€ ParentDocumentRetriever-InsideRAG.ipynb
â”‚   â””â”€â”€ MultiVectorRetriever-InsideRAG.ipynb
â”‚
â”œâ”€â”€ ğŸ§  Memory (Langchain 0.1)/
â”‚   â”œâ”€â”€ CombinedMemoryExample.ipynb
â”‚   â”œâ”€â”€ ConversationBufferMemoryExample.ipynb
â”‚   â”œâ”€â”€ MessagesPlaceholderWithMemoryExample.ipynb
â”‚   â”œâ”€â”€ ConversationSummaryMemoryExample.ipynb
â”‚   â”œâ”€â”€ VectorStoreRetrieverMemoryExample.ipynb
â”‚   â””â”€â”€ SimpleMemory_with_RunnableWithMessageHistory.ipynb
â”‚
â”œâ”€â”€ ğŸ”— Runnable (0.3)/
â”‚   â”œâ”€â”€ Runnable-Based-RAG-Pipeline.ipynb
â”‚   â””â”€â”€ Runnable_for_LLM_PipelineWithMessageHistory.ipynb
â”‚
â”œâ”€â”€ ğŸ”— Chains/
â”‚   â””â”€â”€ [Coming Soon]
â”‚
â”œâ”€â”€ ğŸ› ï¸ Tools/
â”‚   â”œâ”€â”€ Tavily_Search_Extract_Tool.ipynb
â”‚   â”œâ”€â”€ JSON_AgentToolkit_Tool.ipynb
â”‚   â”œâ”€â”€ YahooFinanceNews_Stock_Tools.ipynb
â”‚   â”œâ”€â”€ QuipDocumentGenerator_Tool.ipynb
â”‚   â”œâ”€â”€ NutritionAI_CustomNutritionAI_Tool.ipynb
â”‚   â””â”€â”€ CustomEmailOperations_Tool.ipynb
â”‚
â””â”€â”€ ğŸ¤– LLMs/
    â””â”€â”€ [Coming Soon]
```

## ğŸ” Available Components

### ğŸ“„ Document Loaders & Text Splitters

Transform diverse document formats into LLM-ready chunks with specialized loaders and intelligent splitting strategies.

#### PDF Processing Pipeline

| **Loader + Splitter** | **Key Features** | **Best For** |
|------------------------|------------------|--------------|
| **PyPDFLoader + SemanticChunker** | â€¢ Semantic coherence scoring<br>â€¢ Context-aware boundaries<br>â€¢ Token optimization | Research papers, long-form content |
| **UnstructuredPDFLoader + RecursiveCharacterTextSplitter** | â€¢ Advanced structure detection<br>â€¢ Memory-efficient processing<br>â€¢ Content preservation | Complex layouts, mixed content |
| **PyMuPDFLoader + RecursiveCharacterTextSplitter** | â€¢ Enhanced PDF parsing<br>â€¢ Metadata extraction<br>â€¢ Performance benchmarks | High-volume PDF processing |

#### Specialized Document Formats

| **Document Type** | **Implementation** | **Capabilities** |
|-------------------|-------------------|------------------|
| **ğŸ“„ Text Files** | TextLoader + SemanticChunker | Plain text processing, narrative boundaries |
| **ğŸ“Š Excel Files** | UnstructuredExcelLoader + CharacterTextSplitter | Multi-sheet processing, tabular data extraction |
| **ğŸ“ Word Documents** | UnstructuredWordDocumentLoader + RecursiveCharacterTextSplitter | Rich formatting, embedded content handling |
| **ğŸ¯ PowerPoint** | UnstructuredPowerPointLoader + CharacterTextSplitter | Slide extraction, speaker notes processing |
| **ğŸ”§ JSON Data** | JSONLoader + RecursiveJsonSplitter | Schema-aware splitting, hierarchical preservation |
| **ğŸŒ HTML/Web** | BSHTMLLoader / UnstructuredHTMLLoader | Web scraping, content-focused extraction |
| **ğŸ“‹ Markdown** | UnstructuredMarkdownLoader + MarkdownHeaderTextSplitter | Structure-preserving, header-based chunking |

### ğŸ§¬ Embedding Models

**Base Embedding Models** - Comprehensive performance analysis across leading embedding architectures:

| **Model Family** | **Variants** | **Strengths** |
|------------------|--------------|---------------|
| **MiniLM** | L6-v2, LAgainst-v2 | Fast inference, balanced performance |
| **MPNet** | all-mpnet-base-v2, multi-qa-mpnet-base-dot-v1 | Superior semantic understanding |
| **Multilingual** | gte-multilingual-base | Cross-language retrieval |
| **Commercial** | OpenAI text-embedding-3-large, Cohere v3.0 | State-of-the-art accuracy |
| **Specialized** | GIST-Embedding-v0, msmarco-distilbert-cos-v5 | Domain-specific optimization |

**Performance Metrics for Embedding Models**:

| **Metric** | **Purpose** | **Calculation Formula** |
|------------|-------------|-------------------------|
| **MRR (Mean Reciprocal Rank)** | Measures ranking quality by prioritizing the first relevant result | `MRR = (1/Q) * Î£(1/rank_i)`, where `Q` is number of queries, `rank_i` is position of first relevant result for query `i` |
| **MAP (Mean Average Precision)** | Evaluates precision across all relevant results | `MAP = (1/Q) * Î£(AP_q)`, where `AP_q = Î£(Precision@k * rel(k)) / # relevant docs`, `rel(k)` is 1 if item at rank `k` is relevant, else 0 |
| **NDCG (Normalized Discounted Cumulative Gain)** | Assesses ranking quality with graded relevance, emphasizing higher ranks | `NDCG = DCG / IDCG`, where `DCG = Î£((2^rel_i - 1) / log2(i+1))`, `IDCG` is ideal DCG |
| **Top-1 Accuracy** | Measures if the top result is relevant | `Accuracy = (# correct top-1 predictions) / (# queries)` |
| **Average Spearman** | Evaluates correlation between predicted and true rankings | `Spearman = 1 - (6 * Î£d_i^2) / (n * (n^2 - 1))`, where `d_i` is difference in ranks, `n` is number of items |

**Reranker Models** - Boost retrieval precision with advanced reranking:

| **Model** | **Architecture** | **Use Case** |
|-----------|------------------|--------------|
| **ms-marco-MiniLM-L-6-v2** | Lightweight transformer | Production environments, speed-critical |
| **ms-marco-MiniLM-L-12-v2** | Enhanced transformer | Balanced accuracy-speed tradeoff |
| **ModernBERT-base-gooaq-bce** | Modern BERT + BCE | Maximum accuracy, research applications |
| **ğŸ†• quora-distilroberta-base** | DistilRoBERTa architecture | Question similarity, community Q&A |
| **ğŸ†• stsb-roberta-large** | Large RoBERTa model | Semantic textual similarity tasks |
| **ğŸ†• qnli-electra-base** | ELECTRA-based model | Natural language inference, Q&A |

### ğŸ§² Vector Databases

Production-ready integrations with leading vector storage solutions:

<div align="center">

| **FAISS** | **ChromaDB** | **Weaviate** |
|-----------|--------------|--------------|
| **Meta's similarity search** | **Local-first vector DB** | **Production vector DB** |
| High-performance indexing | Simple setup & deployment | GraphQL API, cloud-ready |

| **Qdrant** | **Pinecone** | **Redis** |
|------------|--------------|-----------|
| **Open-source vector DB** | **Managed vector service** | **In-memory vector search** |
| Self-hosted flexibility | Serverless scaling | Ultra-low latency |

</div>

### ğŸ“ Prompt Templates

Master prompt engineering with comprehensive templating solutions, now evaluated with performance metrics for effectiveness.

| **Template Type** | **Purpose** | **Key Features** |
|-------------------|-------------|------------------|
| **PromptTemplate** | Basic text prompts | Variable substitution, input validation |
| **ChatPromptTemplate** | Conversation handling | Multi-turn context, role management |
| **ChatMessagePromptTemplate** | Individual messages | Role-based formatting, dynamic composition |
| **ğŸ†• ImagePromptTemplate** | Vision model prompts | Image + text prompting, multimodal workflows |
| **ğŸ†• PipelinePromptTemplate** | Complex prompt chains | Multi-stage processing, prompt composition |

**Performance Metrics for Prompt Templates**:

| **Metric** | **Purpose** | **Calculation Formula** |
|------------|-------------|-------------------------|
| **Exact Match (EM)** | Measures if the model's answer exactly matches the ground truth (case-insensitive) | `EM = 1` if prediction equals ground truth, else `0` |
| **F1 (Token Overlap)** | Evaluates precision and recall of shared tokens between prediction and ground truth | `F1 = 2 * (Precision * Recall) / (Precision + Recall)`, where `Precision = |P âˆ© G| / |P|`, `Recall = |P âˆ© G| / |G|` |
| **Accuracy (Overlap-based)** | Checks for any token overlap between prediction and ground truth | `Accuracy = 1` if `|P âˆ© G| > 0`, else `0` |
| **Pass@k** | Indicates if at least one of k generated answers matches ground truth | `Pass@k = 1` if any of top `k` predictions match, else `0` |
| **METEOR** | Assesses semantic similarity, considering synonyms, word order, and stemming | Uses METEOR algorithm: aligns prediction and ground truth, computes weighted harmonic mean of precision and recall |

### ğŸ”§ Output Parsers

Transform raw LLM outputs into structured, usable formats with comprehensive parsing strategies designed for production reliability and type safety.

#### ğŸ¯ Why Output Parsers Matter

Raw LLM outputs are often unstructured strings that need transformation into specific formats for downstream applications. Output parsers bridge this gap by providing:

- **ğŸ”’ Type Safety** - Guaranteed output formats with validation
- **ğŸ“Š Structured Data** - Convert text to JSON, XML, or custom formats
- **ğŸ›¡ï¸ Error Handling** - Robust parsing with fallback mechanisms
- **ğŸ”„ Consistency** - Standardized output formats across different LLMs
- **âš¡ Performance** - Optimized parsing for production workflows

#### ğŸ¨ Parser Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           OUTPUT PARSER PIPELINE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Raw LLM Output â”€â”€â–º [Parser] â”€â”€â–º [Validation] â”€â”€â–º [Transformation] â”€â”€â–º Resultâ”‚
â”‚         â”‚              â”‚            â”‚                  â”‚                    â”‚
â”‚         â”‚              â–¼            â–¼                  â–¼                    â”‚
â”‚         â”‚         [Format       [Schema            [Type                   â”‚
â”‚         â”‚          Detection]    Validation]       Casting]                â”‚
â”‚         â”‚              â”‚            â”‚                  â”‚                    â”‚
â”‚         â”‚              â–¼            â–¼                  â–¼                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â–º [Error Recovery & Retry Logic] â—„â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        PARSER TYPES & USE CASES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ ğŸ“ STRING PARSER    â”‚ ğŸ”§ JSON PARSER     â”‚ ğŸ“‹ XML PARSER                   â”‚
â”‚ Clean & format text â”‚ Structured data    â”‚ Hierarchical data               â”‚
â”‚ Remove artifacts    â”‚ API responses      â”‚ Configuration files             â”‚
â”‚ Standardize output  â”‚ Database records   â”‚ Markup processing               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ğŸ”§ Available Parser Implementations

| **Parser Type** | **Implementation** | **Key Features** | **Best For** |
|-----------------|-------------------|------------------|--------------|
| **StrOutputParser** | String processing & cleanup | â€¢ Text normalization<br>â€¢ Whitespace handling<br>â€¢ Encoding standardization<br>â€¢ Content filtering | â€¢ Simple text responses<br>â€¢ Content summarization<br>â€¢ Text classification tasks<br>â€¢ Basic Q&A systems |
| **JSONOutputParser** | JSON structure parsing | â€¢ Schema validation<br>â€¢ Type enforcement<br>â€¢ Nested object handling<br>â€¢ Error recovery | â€¢ API integrations<br>â€¢ Structured data extraction<br>â€¢ Database operations<br>â€¢ Complex data workflows |
| **XMLOutputParser** | XML document parsing | â€¢ Hierarchical data extraction<br>â€¢ Namespace support<br>â€¢ Attribute parsing<br>â€¢ Document validation | â€¢ Configuration management<br>â€¢ Markup processing<br>â€¢ Legacy system integration<br>â€¢ Structured document workflows |

#### ğŸ¯ Parser Use Cases & Examples

**ğŸ“ String Parser Applications**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         STRING PARSER SCENARIOS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ ğŸ’¬ CHAT RESPONSES     â”‚ ğŸ“„ CONTENT GENERATION â”‚ ğŸ¯ CLASSIFICATION           â”‚
â”‚ Clean conversational â”‚ Article & blog writing â”‚ Sentiment analysis          â”‚
â”‚ text for display     â”‚ Social media content   â”‚ Category assignment         â”‚
â”‚                      â”‚                        â”‚                             â”‚
â”‚ ğŸ” SEARCH QUERIES     â”‚ ğŸ“Š REPORT GENERATION   â”‚ ğŸŒ TRANSLATION              â”‚
â”‚ Query preprocessing  â”‚ Executive summaries    â”‚ Language conversion         â”‚
â”‚ Intent extraction    â”‚ Technical documentationâ”‚ Localization workflows      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ”§ JSON Parser Applications**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          JSON PARSER SCENARIOS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ ğŸŒ API RESPONSES      â”‚ ğŸ“Š DATA EXTRACTION     â”‚ ğŸ”— INTEGRATION              â”‚
â”‚ REST API integration â”‚ Entity recognition     â”‚ Webhook processing          â”‚
â”‚ Service orchestrationâ”‚ Structured information â”‚ Multi-system workflows      â”‚
â”‚                      â”‚                        â”‚                             â”‚
â”‚ ğŸ“‹ FORM PROCESSING    â”‚ ğŸ¯ DECISION TREES      â”‚ ğŸ“ˆ ANALYTICS                â”‚
â”‚ User input validationâ”‚ Logic flow execution   â”‚ Metrics collection          â”‚
â”‚ Dynamic form creationâ”‚ Conditional processing â”‚ Performance tracking        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ“‹ XML Parser Applications**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          XML PARSER SCENARIOS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ âš™ï¸ CONFIGURATION      â”‚ ğŸ“„ DOCUMENT PROCESSING â”‚ ğŸ”„ DATA TRANSFORMATION     â”‚
â”‚ Settings management  â”‚ Rich text formatting   â”‚ Format conversion           â”‚
â”‚ Environment configs  â”‚ Markup preservation    â”‚ Legacy system migration     â”‚
â”‚                      â”‚                        â”‚                             â”‚
â”‚ ğŸŒ WEB SCRAPING       â”‚ ğŸ“Š REPORT GENERATION   â”‚ ğŸ—‚ï¸ METADATA EXTRACTION      â”‚
â”‚ HTML content parsing â”‚ Structured reporting   â”‚ Document properties         â”‚
â”‚ Data extraction      â”‚ Template processing    â”‚ Content cataloging          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ” Retrievers

Advanced retrieval strategies for sophisticated RAG implementations with comprehensive end-to-end examples, now evaluated with performance metrics.

| **Retriever Type** | **Implementation** | **Key Features** | **Best For** |
|-------------------|-------------------|------------------|--------------|
| **VectorStoreRetrieverMMR** | Maximum Marginal Relevance in RAG | â€¢ Diversity optimization<br>â€¢ Relevance-diversity balance<br>â€¢ Complete RAG pipeline integration | Avoiding redundant results, diverse content retrieval |
| **ParentDocumentRetriever** | Hierarchical document retrieval in RAG | â€¢ Parent-child document relationships<br>â€¢ Context preservation<br>â€¢ Full RAG workflow demonstration | Long documents, maintaining context while chunking |
| **MultiVectorRetriever** | Multiple vector representation in RAG | â€¢ Multiple embedding strategies<br>â€¢ Enhanced retrieval accuracy<br>â€¢ End-to-end RAG implementation | Complex documents, multi-aspect retrieval |

**Performance Metrics for Retrievers**:

| **Metric** | **Purpose** | **Calculation Formula** |
|------------|-------------|-------------------------|
| **Precision@10** | Measures proportion of top 10 retrieved documents that are relevant | `Precision@10 = (# relevant docs in top 10) / 10` |
| **Recall@10** | Measures proportion of all relevant documents retrieved in top 10 | `Recall@10 = (# relevant docs in top 10) / (# total relevant docs)` |
| **F1 Score** | Balances precision and recall | `F1 = 2 * (Precision * Recall) / (Precision + Recall)` |
| **MRR (Mean Reciprocal Rank)** | Prioritizes ranking of first relevant document | `MRR = (1/Q) * Î£(1/rank_i)`, where `rank_i` is position of first relevant result |
| **Hit@5** | Indicates if at least one relevant document is in top 5 | `Hit@5 = 1` if top 5 contains a relevant doc, else `0` |
| **Faithfulness** | Evaluates if retrieved content aligns with ground truth | `Faithfulness = (# retrieved docs consistent with ground truth) / (# retrieved docs)` |

Each retriever notebook includes:
- **ğŸ”„ Complete RAG Pipeline** - From document loading to final answer generation
- **ğŸ“Š Performance Metrics** - Retrieval quality and response evaluation
- **âš™ï¸ Configurable Parameters** - Fine-tune retrieval strategies
- **ğŸ¯ Real-world Examples** - Practical use cases and implementations
- **ğŸ“ˆ Comparative Analysis** - Benchmarking against baseline retrievers

### ğŸ§  Memory Systems (LangChain 0.1)

Implement sophisticated conversation memory and context management for stateful LLM applications with comprehensive memory strategies:

#### Core Memory Types

| **Memory Type** | **Implementation** | **Key Features** | **Best For** |
|-----------------|-------------------|------------------|--------------|
| **ConversationBufferMemory** | Basic conversation history storage | â€¢ Simple message buffering<br>â€¢ Automatic conversation tracking<br>â€¢ Memory size management | Chatbots, basic conversational AI |
| **ConversationSummaryMemory** | Intelligent conversation summarization | â€¢ Automatic conversation summarization<br>â€¢ Token-efficient storage<br>â€¢ Context compression | Long conversations, token optimization |
| **CombinedMemory** | Multiple memory types integration | â€¢ Multi-modal memory combination<br>â€¢ Flexible memory strategies<br>â€¢ Advanced context preservation | Complex applications, multi-context scenarios |

#### Advanced Memory Integration

| **Memory Type** | **Implementation** | **Key Features** | **Best For** |
|-----------------|-------------------|------------------|--------------|
| **VectorStoreRetrieverMemory** | Vector-based memory retrieval | â€¢ Semantic memory search<br>â€¢ Long-term context storage<br>â€¢ Similarity-based recall | Knowledge-intensive applications, semantic search |
| **MessagesPlaceholderWithMemory** | Template-based memory integration | â€¢ Prompt template integration<br>â€¢ Dynamic message insertion<br>â€¢ Structured conversation flow | Structured dialogues, guided conversations |
| **SimpleMemory with RunnableWithMessageHistory** | Modern LangChain memory patterns | â€¢ Runnable-based architecture<br>â€¢ Session management<br>â€¢ Streamlined memory handling | Production applications, modern LangChain workflows |

**Comprehensive Memory Features:**
- **ğŸ’¬ Conversation Continuity** - Maintain context across multiple interactions with various retention strategies
- **ğŸ”„ State Management** - Persistent conversation state handling with session support
- **âš™ï¸ Configurable Retention** - Control memory size, summarization, and retention policies
- **ğŸ“ Message Formatting** - Structured conversation history management and template integration
- **ğŸ¯ Semantic Recall** - Vector-based memory search for intelligent context retrieval
- **ğŸ“Š Memory Optimization** - Token-efficient storage through summarization and compression
- **ğŸ”— Modern Integration** - Compatible with latest LangChain patterns and runnable architectures

**Memory Strategy Comparison:**
- **Buffer Memory**: Best for short conversations requiring full history
- **Summary Memory**: Ideal for long conversations needing context compression
- **Vector Memory**: Perfect for knowledge-intensive apps requiring semantic search
- **Combined Memory**: Optimal for complex scenarios requiring multiple memory types
- **Runnable Memory**: Recommended for modern production applications

*Note: These implementations use LangChain 0.1 memory patterns for backward compatibility and established workflows, with examples ranging from basic buffer storage to advanced vector-based semantic memory.*

### ğŸ”— Runnable Architecture (LangChain 0.3)

Master the modern **Runnable** paradigm - LangChain's revolutionary approach to building composable, type-safe, and highly performant LLM applications through functional programming principles.

#### ğŸ¯ What are Runnables?

**Runnables** represent LangChain's next-generation architecture that treats every component as a composable, functional unit. Think of them as intelligent building blocks that can be seamlessly connected, parallelized, and orchestrated to create sophisticated AI workflows.

#### ğŸ—ï¸ Runnable Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           RUNNABLE ECOSYSTEM                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Input Data â”€â”€â–º [Runnable A] â”€â”€â–º [Runnable B] â”€â”€â–º [Runnable C] â”€â”€â–º Output   â”‚
â”‚       â”‚              â”‚              â”‚              â”‚                       â”‚
â”‚       â”‚              â–¼              â–¼              â–¼                       â”‚
â”‚       â”‚         [Parallel      [Conditional   [Memory                     â”‚
â”‚       â”‚          Execution]     Routing]      Integration]                 â”‚
â”‚       â”‚              â”‚              â”‚              â”‚                       â”‚
â”‚       â”‚              â–¼              â–¼              â–¼                       â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â–º [Error Handling & Monitoring] â—„â”€â”€â”€â”˜                       â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        CORE RUNNABLE FEATURES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ ğŸ”„ COMPOSABILITY    â”‚ ğŸš€ ASYNC SUPPORT   â”‚ ğŸ¯ TYPE SAFETY                  â”‚
â”‚ Chain components    â”‚ Non-blocking ops   â”‚ Runtime validation              â”‚
â”‚ seamlessly          â”‚ & parallel exec    â”‚ & error prevention              â”‚
â”‚                     â”‚                    â”‚                                 â”‚
â”‚ ğŸ“Š STREAMING        â”‚ ğŸ” OBSERVABILITY   â”‚ âš¡ PERFORMANCE                  â”‚
â”‚ Real-time output    â”‚ Built-in logging   â”‚ Optimized execution             â”‚
â”‚ & progressive resp  â”‚ & monitoring       â”‚ & resource mgmt                 â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ğŸš€ Runnable Pipeline Patterns

```
ğŸ“‹ SEQUENTIAL PIPELINE (Linear Execution)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document â”‚â”€â”€â–º â”‚ Embedder â”‚â”€â”€â–º â”‚ Retrieverâ”‚â”€â”€â–º â”‚   LLM    â”‚
â”‚ Loader   â”‚    â”‚          â”‚    â”‚          â”‚    â”‚ Response â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”€ PARALLEL PIPELINE (Concurrent Execution)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector   â”‚â”€â”€â–º â”‚          â”‚
â”‚ Search   â”‚    â”‚ Response â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œâ”€â–ºâ”‚ Merger   â”‚â”€â”€â–º Final Output
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚          â”‚
â”‚ Keyword  â”‚â”€â”˜  â”‚          â”‚
â”‚ Search   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§  MEMORY-ENHANCED PIPELINE (Stateful Execution)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User   â”‚â”€â”€â–º â”‚ Message  â”‚â”€â”€â–º â”‚   LLM    â”‚â”€â”€â–º â”‚ Response â”‚
â”‚  Input   â”‚    â”‚ History  â”‚    â”‚Pipeline  â”‚    â”‚+ Memory  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–²                              â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           Memory Update Loop
```

#### ğŸ›ï¸ Available Runnable Implementations

| **Runnable Type** | **Implementation** | **Key Features** | **Architecture Benefits** |
|-------------------|-------------------|------------------|---------------------------|
| **Runnable-Based RAG Pipeline** | Complete RAG workflow using Runnable architecture | â€¢ Modular component design<br>â€¢ Type-safe data flow<br>â€¢ Async/streaming support<br>â€¢ Error handling & monitoring | â€¢ Composable components<br>â€¢ Easy testing & debugging<br>â€¢ Performance optimization<br>â€¢ Scalable architecture |
| **Runnable LLM Pipeline with MessageHistory** | Conversational LLM with persistent memory | â€¢ Stateful conversation management<br>â€¢ Message history integration<br>â€¢ Session-based memory<br>â€¢ Streaming responses | â€¢ Memory state isolation<br>â€¢ Concurrent session handling<br>â€¢ Efficient context management<br>â€¢ Production-ready patterns |

#### ğŸ”§ Runnable Core Concepts

**ğŸ¯ Composability**
```python
# Traditional Approach (Complex & Rigid)
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run(input_data)

# Runnable Approach (Simple & Flexible)
pipeline = prompt | llm | output_parser
result = pipeline.invoke(input_data)
```

**âš¡ Streaming & Async**
```python
# Real-time streaming responses
async for chunk in pipeline.astream(input_data):
    print(chunk, end="")

# Batch processing with parallelization
results = await pipeline.abatch([input1, input2, input3])
```

**ğŸ”„ Error Handling & Retry**
```python
# Built-in error handling with fallbacks
pipeline_with_fallback = primary_pipeline.with_fallbacks([
    fallback_pipeline_1,
    fallback_pipeline_2
])
```

#### ğŸ“Š Runnable Advantages Over Traditional Chains

| **Aspect** | **Traditional Chains** | **Runnable Architecture** |
|------------|------------------------|---------------------------|
| **Composition** | Complex inheritance patterns | Simple pipe operators (`|`) |
| **Type Safety** | Runtime errors common | Compile-time validation |
| **Performance** | Sequential execution | Parallel & async support |
| **Debugging** | Black box behavior | Transparent data flow |
| **Testing** | Difficult to unit test | Easy component isolation |
| **Streaming** | Limited support | Native streaming capabilities |
| **Memory Management** | Manual state handling | Built-in session management |
| **Error Handling** | Custom implementations | Standardized patterns |

#### ğŸ“ Learning Path for Runnables

1. **Start with Runnable-Based RAG Pipeline** - Understand core concepts and composition patterns
2. **Explore MessageHistory Integration** - Learn stateful applications and memory management
3. **Practice Async & Streaming** - Master performance optimization techniques
4. **Build Custom Runnables** - Create your own composable components

**ğŸ”® Why Runnables Matter:**
- **Future-Proof Architecture** - Built for LangChain's long-term vision
- **Industry Standards** - Following functional programming best practices
- **Production Ready** - Designed for enterprise-scale applications
- **Developer Experience** - Simplified debugging and maintenance

*Note: Runnable architecture represents LangChain 0.3+ paradigms and is the recommended approach for new applications, offering superior performance, maintainability, and developer experience.*

### ğŸ› ï¸ LangChain Tools

Extend LLM capabilities with powerful external integrations:

#### Search & Information Tools
| **Tool** | **Functionality** | **Use Cases** |
|----------|-------------------|---------------|
| **Tavily Search & Extract** | Real-time web search + content extraction | Research automation, fact-checking |

#### Data Processing Tools
| **Tool** | **Functionality** | **Use Cases** |
|----------|-------------------|---------------|
| **JSON AgentToolkit** | JSON manipulation & querying | API integration, data transformation |
| **ğŸ†• QuipDocumentGenerator** | Automated document creation | Report generation, content workflows |

#### Financial & Market Tools
| **Tool** | **Functionality** | **Use Cases** |
|----------|-------------------|---------------|
| **Yahoo Finance News & Stock** | Market data & financial news | Investment research, market analysis |

#### Specialized AI Tools
| **Tool** | **Functionality** | **Use Cases** |
|----------|-------------------|---------------|
| **ğŸ†• NutritionAI & CustomNutritionAI** | Nutritional analysis & recommendations | Health apps, dietary planning |
| **ğŸ†• CustomEmailOperations** | Email automation & processing | Communication workflows, CRM integration |

## ğŸš€ Getting Started

### Prerequisites
```bash
pip install langchain langchain-community langchain-openai
pip install faiss-cpu chromadb sentence-transformers
```

### Quick Start
1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/LangChain-Playlist.git
   cd LangChain-Playlist
   ```

2. **Open any notebook in Google Colab**
   - Each notebook contains installation commands
   - All examples are self-contained
   - Interactive parameters for experimentation

3. **Start with the basics**
   - Begin with `Loader and Splitters/` for document processing
   - Move to `Embedding Models/` for vector representations
   - Progress to `VectorDB's/` for storage solutions
   - Master `PromptsTemplates/` for prompt engineering
   - **NEW: Explore `Output Parsers/` for structured output handling**
   - Explore `Retrievers/` for advanced retrieval strategies
   - Learn `Memory (Langchain 0.1)/` for conversation state management
   - Master `Runnable (0.3)/` for modern LangChain architecture patterns

## ğŸ“Š Notebook Features

Each notebook includes:

- **ğŸ¯ Clear Learning Objectives** - What you'll accomplish
- **ğŸ“š Concept Explanations** - Theory behind implementations  
- **âš™ï¸ Interactive Parameters** - Modify and experiment easily
- **ğŸ“ˆ Performance Metrics** - Visual comparisons and benchmarks
- **ğŸ” Troubleshooting Guides** - Common issues and solutions
- **ğŸ’¡ Best Practices** - Production deployment tips

## ğŸ”„ Upcoming Features

| **Category** | **Components** | **ETA** |
|--------------|----------------|---------|
| **ğŸ¤– LLM Integration** | GPT-4, Claude, Llama, Gemini implementations | Q2 2024 |
| **ğŸ§ª Advanced RAG Pipelines** | Multi-agent RAG, conditional retrieval, hybrid approaches | Q3 2024 |
| **ğŸ“Š Benchmarking Suite** | Automated performance testing & comparison | Q3 2024 |
| **ğŸ”§ Agent Frameworks** | Multi-agent systems, tool routing | Q4 2024 |

## ğŸ¤ Contributing

We welcome contributions from the community!

### Ways to Contribute
- **â­ Star this repository** to show support
- **ğŸ› Report bugs** or request clarifications
- **ğŸ’¡ Suggest new components** or improvements
- **ğŸ“ Submit pull requests** with new implementations
- **ğŸ“š Improve documentation** and examples

### Contribution Guidelines
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Follow the existing notebook structure and naming conventions
4. Include comprehensive documentation and examples
5. Test your implementation thoroughly
6. Submit a pull request with detailed description

## ğŸ“œ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **LangChain Team** for the incredible framework
- **Open Source Community** for embedding models and vector databases
- **Contributors** who help improve this resource

## ğŸ“ Support

- **ğŸ“– Documentation**: Check individual notebook READMEs
- **ğŸ’¬ Discussions**: Use GitHub Discussions for questions
- **ğŸ› Issues**: Report bugs via GitHub Issues
- **ğŸ“§ Contact**: Open an issue for direct communication

---

<div align="center">
  
**âš¡ Ready to master LangChain? Start with any notebook and build your LLM expertise step by step! âš¡**

*Built with â¤ï¸ for the LangChain community*

[![GitHub stars](https://img.shields.io/github/stars/dhruvsh1997/LangchainExperiments?style=social)](https://github.com/dhruvsh1997/LangchainExperiments)
[![GitHub forks](https://img.shields.io/github/forks/dhruvsh1997/LangchainExperiments?style=social)](https://github.com/dhruvsh1997/LangchainExperiments)

</div>