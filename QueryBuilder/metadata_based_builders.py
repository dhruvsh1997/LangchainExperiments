# -*- coding: utf-8 -*-
"""Metadata-based Builders.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RVkOFiGPdJVsDCSiXpYt8kGYN7dy-Jbs

Metadata-based Builders – Add filters or tags (e.g., dates, topic keywords).
"""

# ===================== INSTALL DEPENDENCIES =====================
!pip install -q langchain chromadb sentence-transformers groq langchain-community langchain_groq lark pypdf

# =================== IMPORTS ===================
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import get_query_constructor_prompt, StructuredQueryOutputParser
from langchain.chains.query_constructor.schema import AttributeInfo
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableMap, RunnablePassthrough
from langchain_groq import ChatGroq
from google.colab import userdata

# ================== LOAD & SPLIT PDF ==================
# Load PDF
loader = PyPDFLoader("/content/solid-python.pdf")
raw_docs = loader.load()

# Add metadata to each document
for doc in raw_docs:
    doc.metadata["topic"] = "python"
    doc.metadata["year"] = 2021

# Split into chunks
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(raw_docs)

# ================== EMBEDDINGS + CHROMA ==================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embedding_model,
    collection_name="solid_docs"
)

# ================== DEFINE LLM ==================
from google.colab import userdata
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    api_key=userdata.get("GROQ_API_KEY")
)
llm

# ================== METADATA-BASED QUERY CONSTRUCTOR ==================
import lark
# Define metadata structure
attribute_info = [
    AttributeInfo(name="topic", description="The topic of the document", type="string"),
    AttributeInfo(name="year", description="The publication year", type="integer"),
]

document_contents = "Sections from a PDF on advanced Python programming concepts."

# Prompt and output parser
prompt = get_query_constructor_prompt(document_contents, attribute_info)
output_parser = StructuredQueryOutputParser.from_components()

# Build Query Construction Chain (LLM → StructuredQuery)
query_constructor_chain = prompt | llm | output_parser

# ================== PROMPT FOR ANSWERING ==================
from langchain_core.prompts import PromptTemplate

rag_prompt = PromptTemplate.from_template(
    "Use the following context to answer the question:\n\n{context}\n\nQuestion: {question}"
)

# ================== Retrivers =============================
retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5})

# ================== BUILD FULL RAG CHAIN ==================

query_runnable = RunnableLambda(lambda x: {"question": x["question"]})
retrieve_runnable = query_runnable | RunnableLambda(lambda inp: retriever.get_relevant_documents(inp["question"]))

rag_chain = (
    RunnableMap({
        "context": retrieve_runnable,
        "question": RunnablePassthrough()
    })
    | rag_prompt
    | llm
)

# ================== ASK A QUESTION ==================
question = "Show me tips on Python from the year 2021"

response = rag_chain.invoke({"question": question})

print("Final Answer:\n")
print(response.content)

