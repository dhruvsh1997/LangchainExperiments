# -*- coding: utf-8 -*-
"""Static Builder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JTxxadOuGt4EbxaXyiGBRc_aT6EJ864w

Static Builders â€“ Simple templates, like f"Question: {user_input} | Task: Summarization".
"""

# ===================== INSTALL DEPENDENCIES =====================
!pip install -q langchain sentence-transformers faiss-cpu pypdf groq langchain-community langchain-groq

# ================== IMPORTS ==================
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_groq import ChatGroq

# ================== LOAD & SPLIT PDF ==================
loader = PyPDFLoader("/content/solid-python.pdf")
documents = loader.load_and_split()

splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)

# ================== EMBEDDINGS + VECTORSTORE ==================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embedding_model)

# Standard retriever
retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5})

# ================== DEFINE LLM ==================
from google.colab import userdata
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    api_key=userdata.get("GROQ_API_KEY")
)
llm

# ================== STATIC QUERY BUILDER ==================
def static_query_builder(user_question):
    return f"Task: Answer the question precisely | Question: {user_question}"

# ================== PROMPT TEMPLATE ==================
prompt = PromptTemplate.from_template(
    "Use the following context to answer the question:\n\n{context}\n\nQuestion: {question}"
)

# ================== RUN THE STATIC RAG PIPELINE ==================

def run_static_rag(user_question):
    # Step 1: Apply static builder
    static_query = static_query_builder(user_question)

    # Step 2: Retrieve relevant documents using static query
    retrieved_docs = retriever.get_relevant_documents(static_query)

    # Step 3: Prepare context for final prompt
    context = "\n\n".join(doc.page_content for doc in retrieved_docs)

    # Step 4: Format prompt
    final_prompt = prompt.format(context=context, question=user_question)

    # Step 5: Get LLM answer
    response = llm.invoke(final_prompt)
    return response.content

# ================== ASK A QUESTION ==================
question = "What is the main objective of the document explain in emglish?"
answer = run_static_rag(question)

print("Answer:\n")
print(answer)



