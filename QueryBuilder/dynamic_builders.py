# -*- coding: utf-8 -*-
"""Dynamic Builders.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xahGuHzJBsC1y22uoINfF6T8CY7lFNsz

Dynamic Builders â€“ Use LLMs to rephrase, enrich, or clarify the query.
"""

# ===================== INSTALL DEPENDENCIES =====================
!pip install -q langchain sentence-transformers faiss-cpu pypdf groq langchain-community langchain-groq

# ================== IMPORTS ==================
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq

# ================== LOAD & SPLIT PDF ==================
loader = PyPDFLoader("/content/solid-python.pdf")
documents = loader.load_and_split()

splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)

# ================== EMBEDDINGS + VECTORSTORE ==================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embedding_model)

retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5})

# ================== DEFINE LLM ==================
from google.colab import userdata
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    api_key=userdata.get("GROQ_API_KEY")
)
llm

# ================== DYNAMIC QUERY BUILDER ==================
query_builder_prompt = PromptTemplate.from_template(
    "You are a smart assistant. Rewrite the following user question to make it clearer and more suitable for document retrieval:\n\nOriginal Question: {question}\n\nRewritten Query:"
)

query_rewriter = query_builder_prompt | llm | StrOutputParser()

# ================== FINAL PROMPT TEMPLATE ==================
prompt = PromptTemplate.from_template(
    "Use the following context to answer the question:\n\n{context}\n\nQuestion: {question}"
)

# ================== FINAL DYNAMIC RAG FLOW ==================

def run_dynamic_rag(user_question):
    # Step 1: Dynamically rewrite the query using the LLM
    rewritten_query = query_rewriter.invoke({"question": user_question})

    # Step 2: Retrieve documents based on the rewritten query
    retrieved_docs = retriever.get_relevant_documents(rewritten_query)

    # Step 3: Build context for prompt
    context = "\n\n".join(doc.page_content for doc in retrieved_docs)

    # Step 4: Format final prompt
    final_prompt = prompt.format(context=context, question=user_question)

    # Step 5: Generate final answer from LLM
    response = llm.invoke(final_prompt)
    return response.content

# ================== ASK A QUESTION ==================
question = "Explain Open Close Principle?"
answer = run_dynamic_rag(question)

print("Final Answer:\n")
print(answer)

