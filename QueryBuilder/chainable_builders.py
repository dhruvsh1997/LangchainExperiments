# -*- coding: utf-8 -*-
"""Chainable Builders.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FnDTeXB5mkKQKR-r_9OWvLEPpbXav6Br

### What is a Query Builder (Query Constructor) in RAG?
- A Query Builder is a preprocessing step in the RAG (Retrieval-Augmented Generation) pipeline that transforms the user question into a more effective and retrieval-friendly query. Instead of sending the raw question to the retriever, it enriches or modifies the question (sometimes by adding task context, metadata, or rephrasing) to retrieve better documents from the vector store.

###Why Do We Use a Query Builder?
In RAG, document retrieval quality is crucial. A vague or too-specific question might lead to poor retrieval. A Query Builder helps in:

- Improving retrieval accuracy.

- Making vague queries more specific.

- Adding context from the conversation or prompt.

- Aligning with domain-specific language (e.g., legal, medical).

### How to Use Query Builder in Langchain
- Now let’s implement your RAG pipeline with a Query Builder in Colab. We'll use an LLM-based query constructor to rephrase the question before retrieval.

Chainable Builders – Built using Langchain’s Runnable interfaces. Combine Dynamic Builder with Runnables.
"""

# ===================== INSTALL DEPENDENCIES =====================
!pip install -q langchain sentence-transformers faiss-cpu pypdf groq langchain-community langchain-groq

# ================== IMPORTS ==================
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableMap, RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq

# ================== LOAD & SPLIT PDF ==================
loader = PyPDFLoader("/content/solid-python.pdf")
documents = loader.load_and_split()

splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)

# ================== EMBEDDINGS + VECTORSTORE ==================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embedding_model)
retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5})

# ================== DEFINE LLM ==================
from google.colab import userdata
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    api_key=userdata.get("GROQ_API_KEY")
)
llm

# ================== QUERY BUILDER ==================

# Prompt to improve the question for better retrieval
query_builder_prompt = PromptTemplate.from_template(
    "You are a helpful assistant. Rewrite the following user question to be more clear and retrieval-friendly:\n\nOriginal Question: {question}\n\nImproved Question:"
)

# Use the same LLM to rewrite the query
query_builder_chain = query_builder_prompt | llm | StrOutputParser()
query_builder_chain

# ================== RETRIEVAL USING QUERY BUILDER ==================

# Step 1: Rewrite user question
query_rewrite_runnable = RunnableLambda(lambda x: {"question": x["question"]}) | query_builder_chain

# Step 2: Get relevant docs using rewritten query
retriever_runnable = query_rewrite_runnable | RunnableLambda(lambda q: retriever.get_relevant_documents(q))
retriever_runnable

# ================== FINAL PROMPT TEMPLATE ==================
prompt = PromptTemplate.from_template(
    "Use the following context to answer the question:\n\n{context}\n\nQuestion: {question}"
)

# ================== FINAL RAG CHAIN ==================
rag_chain = (
    RunnableMap({
        "context": retriever_runnable,
        "question": RunnablePassthrough()
    })
    | prompt
    | llm
    | StrOutputParser()
)
rag_chain

# ================== RUN THE CHAIN ==================
question = "Explain solid?"
response = rag_chain.invoke({"question": question})

print("Final Response:\n")
print(response)





