{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNyQan6A7F0ZE8B0bkNrRRJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 4. `CombinedMemory`\n","- **Purpose**: Combines multiple memory types (e.g., buffer + summary, static + conversational).\n","- **Use‚Äëcase**: Agents needing both immediate context and long‚Äëterm knowledge.\n","- **Main difference**: Offers composite memory strategy; e.g.,\n","  static data via `SimpleMemory` + chat history via `ConversationBufferMemory` or `ConversationSummaryMemory`."],"metadata":{"id":"Mp5WvuHlW141"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ms_x58h8W1Q7","executionInfo":{"status":"ok","timestamp":1750171717629,"user_tz":-330,"elapsed":24063,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}},"outputId":"875b940f-4dbf-455f-83c2-0c6f0ab6932c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# ================================\n","# Step 1: Install dependencies\n","# ================================\n","!pip install -q langchain langchain-groq langchain-openai"]},{"cell_type":"code","source":["# ================================\n","# Step 2: Import libraries\n","# ================================\n","from langchain_groq import ChatGroq\n","from langchain_openai import ChatOpenAI\n","from langchain.memory import (\n","    ConversationSummaryMemory,\n","    SimpleMemory,\n","    CombinedMemory\n",")\n","from langchain.prompts.chat import (\n","    ChatPromptTemplate,\n","    SystemMessagePromptTemplate,\n","    HumanMessagePromptTemplate,\n","    MessagesPlaceholder,\n",")\n","from IPython.display import Markdown, display\n","from google.colab import userdata"],"metadata":{"id":"YiHRcKAUXI12","executionInfo":{"status":"ok","timestamp":1750171720635,"user_tz":-330,"elapsed":2968,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Step 3: Setup LLMs (Groq + OpenAI)\n","# ================================\n","groq_key = userdata.get(\"GROQ_API_KEY\")\n","openai_key = userdata.get(\"OPENAI_API_KEY3\")\n","\n","# Groq for response generation\n","llm = ChatGroq(\n","    model=\"llama-3.3-70b-versatile\",\n","    api_key=groq_key,\n","    temperature=0.3,\n",")\n","\n","# OpenAI (or lightweight HuggingFace model) for summarization\n","summarizer_llm = ChatOpenAI(\n","    model=\"gpt-4o-mini\",\n","    api_key=openai_key,\n","    temperature=0.3,\n",")"],"metadata":{"id":"Y0W-peUDXLcc","executionInfo":{"status":"ok","timestamp":1750171733444,"user_tz":-330,"elapsed":7622,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Step 4: Setup CombinedMemory\n","# ================================\n","\n","# Static memory ‚Äî constant data like user profile\n","profile_memory = SimpleMemory(memories={\n","    \"user_name\": \"Dhruv\",\n","    \"user_role\": \"Machine Learning Engineer\"\n","})\n","\n","# Dynamic summary memory ‚Äî for chat history\n","summary_memory = ConversationSummaryMemory(\n","    llm=summarizer_llm,\n","    return_messages=True\n",")\n","\n","# Combine both memories\n","combined_memory = CombinedMemory(\n","    memories=[profile_memory, summary_memory]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7R5QHxaXOqd","executionInfo":{"status":"ok","timestamp":1750171733485,"user_tz":-330,"elapsed":32,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}},"outputId":"c3c27375-017f-4067-ac9b-961806649b34"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-1506891984>:12: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  summary_memory = ConversationSummaryMemory(\n","/usr/local/lib/python3.11/dist-packages/langchain/memory/combined.py:40: UserWarning: When using CombinedMemory, input keys should be so the input is known.  Was not set on llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7c24703ffbd0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7c24703fd4d0>, root_client=<openai.OpenAI object at 0x7c24703ff610>, root_async_client=<openai.AsyncOpenAI object at 0x7c24700d4c10>, model_name='gpt-4o-mini', temperature=0.3, model_kwargs={}, openai_api_key=SecretStr('**********')) chat_memory=InMemoryChatMessageHistory(messages=[]) return_messages=True\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# ================================\n","# Step 5: Define Prompt Template\n","# ================================\n","system_msg = SystemMessagePromptTemplate.from_template(\n","    \"You are a helpful assistant. The user's name is {user_name} and they are a {user_role}. \"\n","    \"Always personalize your replies. Use prior conversation (if any) to respond intelligently.\"\n",")\n","\n","human_msg = HumanMessagePromptTemplate.from_template(\"{input}\")\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    system_msg,\n","    MessagesPlaceholder(variable_name=\"history\"),  # injects summarized history\n","    human_msg\n","])"],"metadata":{"id":"J7E14UnzXTeZ","executionInfo":{"status":"ok","timestamp":1750171743838,"user_tz":-330,"elapsed":9,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Step 6: Chat Loop (Manual)\n","# ================================\n","user_inputs = [\n","    \"Hey, can you explain how a neural network learns?\",\n","    \"Now tell me how backpropagation works.\",\n","    \"Do you think it‚Äôs better than decision trees?\"\n","]\n","\n","for input_text in user_inputs:\n","    # Step 1: Load memory values\n","    memory_vars = combined_memory.load_memory_variables({})\n","\n","    # Step 2: Format messages using both memory parts\n","    messages = chat_prompt.format_messages(\n","        input=input_text,\n","        **memory_vars\n","    )\n","\n","    # Step 3: Invoke Groq for final response\n","    response = llm.invoke(messages)\n","\n","    # Step 4: Save to summary memory only (SimpleMemory doesn't change)\n","    summary_memory.save_context(\n","        {\"input\": input_text},\n","        {\"output\": response.content}\n","    )\n","\n","    # Step 5: Display result\n","    display(Markdown(f\"### ‚ùì User: {input_text}\"))\n","    display(Markdown(f\"**ü§ñ Assistant:** {response.content}\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"s4iA5OZzXXTs","executionInfo":{"status":"ok","timestamp":1750171767448,"user_tz":-330,"elapsed":15070,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}},"outputId":"e02a99e6-bb1f-4ff6-dbb5-e36e633fdae9"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ‚ùì User: Hey, can you explain how a neural network learns?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**ü§ñ Assistant:** Hi Dhruv, nice to chat with you about neural networks. As a Machine Learning Engineer, you're likely familiar with the basics, but I'll provide a detailed overview of how a neural network learns.\n\nA neural network learns through a process called backpropagation, which involves the following steps:\n\n1. **Forward Pass**: The network receives input data, which flows through the layers, performing calculations and transformations. The output is calculated based on the current weights and biases of the network.\n2. **Error Calculation**: The difference between the predicted output and the actual output (target) is calculated using a loss function, such as mean squared error or cross-entropy.\n3. **Backward Pass**: The error is propagated backwards through the network, adjusting the weights and biases at each layer to minimize the loss. This is done using the chain rule of calculus.\n4. **Weight Update**: The weights and biases are updated based on the calculated error and the learning rate, which controls how quickly the network learns.\n5. **Optimization**: The network repeats the forward and backward passes, adjusting the weights and biases to minimize the loss. This process is typically performed using an optimization algorithm, such as stochastic gradient descent (SGD), Adam, or RMSProp.\n\nThe key components that enable a neural network to learn are:\n\n* **Activation functions**: Introduce non-linearity to the network, allowing it to learn complex relationships between inputs and outputs.\n* **Loss functions**: Measure the difference between predicted and actual outputs, guiding the network to minimize the error.\n* **Optimization algorithms**: Adjust the weights and biases to minimize the loss, using techniques such as gradient descent and momentum.\n\nAs a Machine Learning Engineer, you're likely working with popular deep learning frameworks like TensorFlow or PyTorch, which provide efficient implementations of these concepts. Do you have any specific questions about implementing neural networks or optimizing their performance, Dhruv?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ‚ùì User: Now tell me how backpropagation works."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**ü§ñ Assistant:** Dhruv, as a Machine Learning Engineer, you're likely familiar with the basics of neural networks, so I'll dive straight into the details of backpropagation. Backpropagation is an essential algorithm in training neural networks, and it's used to minimize the error between the network's predictions and the actual outputs.\n\nThe backpropagation process involves the following steps:\n\n1. **Forward Pass**: First, the network processes the input data and produces an output. This output is then compared to the actual output to calculate the error.\n2. **Error Calculation**: The error is calculated using a loss function, such as mean squared error or cross-entropy. This error represents the difference between the predicted output and the actual output.\n3. **Backward Pass**: The error is then propagated backwards through the network, adjusting the weights and biases of each layer to minimize the error. This is done using the chain rule of calculus.\n4. **Weight Update**: During the backward pass, the gradients of the loss function with respect to each weight and bias are calculated. These gradients are then used to update the weights and biases to minimize the error.\n\nThe key components that enable backpropagation are:\n\n* **Activation Functions**: These introduce non-linearity into the network, allowing it to learn complex relationships between inputs and outputs.\n* **Loss Functions**: These measure the difference between the predicted output and the actual output, providing a way to evaluate the network's performance.\n* **Optimization Algorithms**: These, such as stochastic gradient descent (SGD), Adam, or RMSProp, are used to update the weights and biases based on the calculated gradients.\n\nIn backpropagation, the gradients are calculated using the following formula:\n\nŒ¥ = (dE/dy) \\* (dy/dz) \\* (dz/dw)\n\nwhere Œ¥ is the gradient of the loss function with respect to the weight, dE/dy is the derivative of the loss function with respect to the output, dy/dz is the derivative of the output with respect to the weighted sum, and dz/dw is the derivative of the weighted sum with respect to the weight.\n\nAs someone who works with frameworks like TensorFlow or PyTorch, you're likely familiar with how these libraries implement backpropagation and provide tools for optimizing neural networks. If you have any specific questions about implementing or optimizing backpropagation in your projects, feel free to ask, Dhruv!"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ‚ùì User: Do you think it‚Äôs better than decision trees?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**ü§ñ Assistant:** Dhruv, as a Machine Learning Engineer, you're likely familiar with the strengths and weaknesses of various algorithms. Backpropagation is a key component of neural networks, which can be more powerful than decision trees in certain scenarios.\n\nNeural networks, trained using backpropagation, can learn complex, non-linear relationships between inputs and outputs, making them particularly well-suited for tasks like image classification, natural language processing, and speech recognition. They can also handle large datasets and high-dimensional feature spaces.\n\nIn contrast, decision trees are often more interpretable and can be more efficient to train, but they can struggle with complex relationships and may not generalize as well to new, unseen data. Additionally, decision trees can be prone to overfitting, especially when dealing with noisy or high-dimensional data.\n\nThat being said, decision trees can be a great choice for certain problems, such as feature selection, data exploration, or when interpretability is crucial. They're also often used as a component of more complex models, like random forests or gradient boosting machines.\n\nSo, it ultimately depends on the specific problem you're trying to solve, Dhruv. If you're working on a task that requires learning complex patterns in data, neural networks trained with backpropagation might be a better choice. But if you need a more interpretable model or are working with a smaller dataset, decision trees could be a better fit.\n\nDo you have a specific project in mind where you're considering using backpropagation or decision trees? I'd be happy to help you weigh the pros and cons."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"DZYfGPtYXZZL"},"execution_count":null,"outputs":[]}]}