{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZpZvKs5C5ZaGjQeaftbzQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 3. `ConversationSummaryMemory`\n","- **Purpose**: Maintains a **running summary of the conversation**, updated after each turn.\n","- **Use‚Äëcase**: Long conversations where full verbatim memory is impractical.\n","- **Main difference**: Compresses past discourse into concise summaries, keeping token usage bounded :contentReference[oaicite:2]{index=2}."],"metadata":{"id":"BWf3cmOASxKK"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"kM_UbAaXSW0p","executionInfo":{"status":"ok","timestamp":1750170891462,"user_tz":-330,"elapsed":14670,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"outputs":[],"source":["# ================================\n","# Step 1: Install dependencies\n","# ================================\n","!pip install -q langchain langchain-groq langchain-community langchain-openai"]},{"cell_type":"code","source":["# ================================\n","# Step 2: Import libraries\n","# ================================\n","from langchain_groq import ChatGroq\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts.chat import (\n","    ChatPromptTemplate,\n","    SystemMessagePromptTemplate,\n","    HumanMessagePromptTemplate,\n","    MessagesPlaceholder,\n",")\n","from langchain.memory import ConversationSummaryMemory\n","from IPython.display import Markdown, display\n","from google.colab import userdata"],"metadata":{"id":"-ff-GX8mSYwZ","executionInfo":{"status":"ok","timestamp":1750171084682,"user_tz":-330,"elapsed":41,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Step 3: Setup LLMs (main + summarizer)\n","# ================================\n","api_key = userdata.get(\"GROQ_API_KEY\")\n","openai_key = userdata.get(\"OPENAI_API_KEY3\")\n","\n","llm = ChatGroq(\n","    model=\"llama-3.3-70b-versatile\",\n","    api_key=api_key,\n","    temperature=0.3,\n",")\n","\n","summarizer_llm = ChatOpenAI(\n","    model=\"gpt-4o-mini\",  # You can also use \"gpt-3.5-turbo\"\n","    api_key=openai_key,\n","    temperature=0.3,\n",")"],"metadata":{"id":"OY6W1Pi-Sa65","executionInfo":{"status":"ok","timestamp":1750171111347,"user_tz":-330,"elapsed":1466,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Step 4: Define Prompt Template\n","# ================================\n","system_msg = SystemMessagePromptTemplate.from_template(\n","    \"You are an intelligent assistant. Use the provided conversation summary to inform your responses.\"\n",")\n","human_msg = HumanMessagePromptTemplate.from_template(\"{input}\")\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    system_msg,\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    human_msg\n","])"],"metadata":{"id":"6N2qDkGHSe_U","executionInfo":{"status":"ok","timestamp":1750171119432,"user_tz":-330,"elapsed":7,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Step 5: Setup ConversationSummaryMemory\n","# ================================\n","# This memory summarizes all prior messages to keep the prompt short\n","memory = ConversationSummaryMemory(\n","    llm=summarizer_llm,\n","    return_messages=True\n",")"],"metadata":{"id":"7hpbrTEbSe8L","executionInfo":{"status":"ok","timestamp":1750171126992,"user_tz":-330,"elapsed":8,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# # ================================\n","# # Step 6: Combine LLM, Prompt, and Memory\n","# # ================================\n","# chat_chain = chat_prompt | llm\n","\n","# chatbot = RunnableWithMessageHistory(\n","#     chat_chain,\n","#     lambda session_id: memory,\n","#     input_messages_key=\"input\",\n","#     history_messages_key=\"history\"\n","# )"],"metadata":{"id":"Gszu5C7PSe4y","executionInfo":{"status":"ok","timestamp":1750170939591,"user_tz":-330,"elapsed":7,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Step 7: Start the Chat Loop\n","# ================================\n","session_id = \"chat-summary-session\"\n","user_inputs = [\n","    \"Hey! Can you explain what gradient descent is?\",\n","    \"Cool. And how is it different from stochastic gradient descent?\",\n","    \"Which one is better for large datasets?\"\n","]\n","\n","for input_text in user_inputs:\n","    # Step 1: Load past memory (summary history)\n","    memory_variables = memory.load_memory_variables({})\n","\n","    # Step 2: Format prompt with memory + new input\n","    messages = chat_prompt.format_messages(\n","        input=input_text,\n","        history=memory_variables[\"history\"]\n","    )\n","\n","    # Step 3: Invoke LLM\n","    response = llm.invoke(messages)\n","\n","    # Step 4: Save this turn to memory\n","    memory.save_context(\n","        {\"input\": input_text},\n","        {\"output\": response.content}\n","    )\n","\n","    # Step 5: Display result\n","    display(Markdown(f\"### ‚ùì User: {input_text}\"))\n","    display(Markdown(f\"**ü§ñ Assistant:** {response.content}\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"H-gHdPL-Se2D","executionInfo":{"status":"ok","timestamp":1750171177802,"user_tz":-330,"elapsed":18680,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}},"outputId":"040786eb-6c5f-43b1-aa2e-4d82e3751d5d"},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ‚ùì User: Hey! Can you explain what gradient descent is?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**ü§ñ Assistant:** Gradient descent is a fundamental concept in machine learning and optimization. It's an iterative algorithm used to minimize the loss or cost function of a model by adjusting its parameters.\n\n**How it works:**\n\n1. **Initialization**: The algorithm starts with an initial set of model parameters (e.g., weights and biases).\n2. **Forward pass**: The model makes predictions using the current parameters.\n3. **Loss calculation**: The difference between the predicted output and the actual output (target) is calculated, resulting in a loss or cost value.\n4. **Backward pass**: The algorithm computes the gradient of the loss with respect to each model parameter. The gradient represents the direction of the steepest ascent.\n5. **Parameter update**: The model parameters are updated by subtracting a fraction of the gradient (scaled by a learning rate) from the current parameters. This moves the parameters in the direction of the negative gradient, which is the direction of the steepest descent.\n6. **Repeat**: Steps 2-5 are repeated until convergence or a stopping criterion is reached.\n\n**Key aspects:**\n\n* **Learning rate**: A hyperparameter that controls the step size of each update. A high learning rate can lead to fast convergence but may also cause oscillations.\n* **Gradient**: The direction of the steepest ascent, which is used to update the parameters.\n* **Convergence**: The algorithm stops when the loss stops improving or reaches a minimum value.\n\nGradient descent is a simple yet powerful algorithm for optimizing model parameters. However, it can be sensitive to the choice of learning rate, initialization, and other hyperparameters.\n\nDo you have any specific questions about gradient descent or its applications?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ‚ùì User: Cool. And how is it different from stochastic gradient descent?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**ü§ñ Assistant:** Stochastic Gradient Descent (SGD) is a variant of the traditional Gradient Descent (GD) algorithm. The key difference between the two lies in how they calculate the gradient of the loss function.\n\nIn traditional Gradient Descent, the gradient is calculated using the entire training dataset at once. This is known as batch gradient descent. The algorithm computes the gradient of the loss function with respect to the model's parameters using all the training examples, and then updates the parameters accordingly.\n\nOn the other hand, Stochastic Gradient Descent uses only one training example at a time to calculate the gradient. The algorithm iterates through the training dataset, one example at a time, and updates the parameters after each example. This process is repeated multiple times, with the algorithm cycling through the entire dataset.\n\nThere's also a third variant, known as Mini-Batch Gradient Descent, which falls somewhere in between. In this approach, the algorithm uses a small batch of training examples (typically between 10 and 1000) to calculate the gradient, rather than using the entire dataset or a single example.\n\nThe advantages of Stochastic Gradient Descent include:\n\n* Faster computation, since the algorithm only needs to process one example at a time\n* Ability to handle large datasets, since the algorithm doesn't need to load the entire dataset into memory\n* Improved robustness to outliers, since the algorithm is less affected by individual noisy examples\n\nHowever, Stochastic Gradient Descent can also have some disadvantages, such as:\n\n* Noisier gradient estimates, since the algorithm is only using a single example to calculate the gradient\n* Potential for slower convergence, since the algorithm may need to iterate through the dataset multiple times to achieve the same level of accuracy as traditional Gradient Descent\n\nIn general, the choice between Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent depends on the specific problem, dataset, and computational resources available. Do you have any further questions about these algorithms or their applications?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ‚ùì User: Which one is better for large datasets?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**ü§ñ Assistant:** For large datasets, Stochastic Gradient Descent (SGD) or Mini-Batch Gradient Descent is generally better than traditional Gradient Descent. Here's why:\n\n1. **Computational Efficiency**: Traditional Gradient Descent requires calculating the gradient of the loss function over the entire dataset, which can be computationally expensive and time-consuming for large datasets. SGD, on the other hand, calculates the gradient using one example at a time, making it much faster.\n2. **Memory Usage**: Traditional Gradient Descent requires loading the entire dataset into memory, which can be a challenge for large datasets. SGD, by contrast, only needs to load one example at a time, making it more memory-efficient.\n3. **Robustness to Outliers**: SGD is more robust to outliers in the data, as the gradient is calculated using one example at a time. This means that outliers will have less impact on the overall gradient calculation.\n\nThat being said, Mini-Batch Gradient Descent is often a good compromise between traditional Gradient Descent and SGD. It calculates the gradient using a small batch of examples (typically 32-128) at a time, which can provide a good balance between computational efficiency and accuracy.\n\nIn general, the choice between SGD and Mini-Batch Gradient Descent depends on the specific problem and dataset. If you have a very large dataset and limited computational resources, SGD might be a good choice. However, if you have a moderately sized dataset and want to balance computational efficiency with accuracy, Mini-Batch Gradient Descent might be a better option.\n\nDo you have any further questions about these algorithms or their applications?"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"IsiY-JlhSezM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Em-EaKASSewJ"},"execution_count":null,"outputs":[]}]}