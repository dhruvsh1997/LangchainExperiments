{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXvvOwtz+HmITfxWldTHzT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 5. `VectorStoreRetrieverMemory`\n","- **Purpose**: Stores chat history in a vector database and retrieves relevant past utterances semantically.\n","- **Use‑case**: Large context windows, agent reflection, or retrieving past conversation highlights.\n","- **Main difference**: Retrieves only semantically relevant history based on current input :contentReference[oaicite:3]{index=3}.\n"],"metadata":{"id":"V8RRHu-4Xocf"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZufyJdjXn72","executionInfo":{"status":"ok","timestamp":1750172595471,"user_tz":-330,"elapsed":13482,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}},"outputId":"7fdc392e-49f0-4283-850c-b3e205d01862"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# ================================\n","# Step 1: Install dependencies\n","# ================================\n","!pip install -q langchain langchain-groq sentence-transformers faiss-cpu langchain-community"]},{"cell_type":"code","source":["# ================================\n","# Step 2: Import Libraries\n","# ================================\n","from langchain_groq import ChatGroq\n","from langchain.prompts.chat import (\n","    ChatPromptTemplate,\n","    SystemMessagePromptTemplate,\n","    HumanMessagePromptTemplate,\n","    MessagesPlaceholder\n",")\n","from langchain.memory import VectorStoreRetrieverMemory\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from IPython.display import Markdown, display\n","from google.colab import userdata"],"metadata":{"id":"QWIQhNuJZ90S","executionInfo":{"status":"ok","timestamp":1750172595543,"user_tz":-330,"elapsed":67,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Step 3: Setup Groq LLM\n","# ================================\n","api_key = userdata.get(\"GROQ_API_KEY\")\n","\n","llm = ChatGroq(\n","    model=\"llama-3.3-70b-versatile\",\n","    api_key=api_key,\n","    temperature=0.3,\n",")"],"metadata":{"id":"oeQrrH7FaApk","executionInfo":{"status":"ok","timestamp":1750172600123,"user_tz":-330,"elapsed":4573,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage\n","phdr=\"\"\"placeholder\"\"\""],"metadata":{"id":"8JezNJWLa5S7","executionInfo":{"status":"ok","timestamp":1750173212219,"user_tz":-330,"elapsed":10,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Step 4: Setup VectorStoreRetrieverMemory\n","# ================================\n","\n","# Step 4.1: Embedding model\n","embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","# Step 4.2: Initialize FAISS vector store\n","vectorstore = FAISS.from_texts([phdr], embedding)\n","\n","memory = VectorStoreRetrieverMemory(\n","    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n","    memory_key=\"history\",  # maps to MessagesPlaceholder\n",")"],"metadata":{"id":"aoxsYYjQaEZo","executionInfo":{"status":"ok","timestamp":1750173215273,"user_tz":-330,"elapsed":2355,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Step 5: Prompt Template with Memory\n","# ================================\n","system_msg = SystemMessagePromptTemplate.from_template(\n","    \"You are a helpful assistant. Recall relevant past conversations using memory.\"\n",")\n","human_msg = HumanMessagePromptTemplate.from_template(\"{input}\")\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    system_msg,\n","    MessagesPlaceholder(variable_name=\"history\"),  # injects retrieved context\n","    human_msg\n","])"],"metadata":{"id":"FONB3xiUaG01","executionInfo":{"status":"ok","timestamp":1750173215282,"user_tz":-330,"elapsed":5,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# Step 6: Chat Loop with Memory\n","# ================================\n","user_inputs = [\n","    \"Can you explain how backpropagation works in neural networks?\",\n","    \"Thanks! What about dropout in training?\",\n","    \"Remind me what you said earlier about backpropagation.\"\n","]\n","\n","for input_text in user_inputs:\n","    # Step 1: Retrieve memory text and convert to message list\n","    retrieved = memory.load_memory_variables({\"input\": input_text})[\"history\"]\n","    if retrieved.strip() == \"placeholder\":\n","        messages = []\n","    else:\n","        messages = [HumanMessage(content=\"Earlier you said: \" + retrieved)]\n","\n","    # Step 2: Format prompt with history and new input\n","    formatted_messages = chat_prompt.format_messages(\n","        input=input_text,\n","        history=messages\n","    )\n","\n","    # Step 3: Get response\n","    response = llm.invoke(formatted_messages)\n","\n","    # Step 4: Save turn into memory\n","    memory.save_context({\"input\": input_text}, {\"output\": response.content})\n","\n","    display(Markdown(f\"### User: {input_text}\"))\n","    display(Markdown(f\"** Assistant:** {response.content}\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"_rnxw2LiaJoH","executionInfo":{"status":"ok","timestamp":1750173251149,"user_tz":-330,"elapsed":7934,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}},"outputId":"217504cb-44a9-4926-a6a9-9202231c2fd4"},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### User: Can you explain how backpropagation works in neural networks?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"** Assistant:** **What is Backpropagation?**\n\nBackpropagation, short for \"backward propagation of errors,\" is an algorithm used to train artificial neural networks. It's a method for supervised learning, where the network is trained on labeled data to minimize the error between its predictions and the actual outputs.\n\n**The Backpropagation Process**\n\nThe backpropagation process involves the following steps:\n\n1. **Forward Pass**: The network processes the input data and produces an output. This output is compared to the actual output (label) to calculate the error.\n2. **Error Calculation**: The error between the predicted output and the actual output is calculated using a loss function, such as mean squared error (MSE) or cross-entropy.\n3. **Backward Pass**: The error is propagated backwards through the network, adjusting the weights and biases of each layer to minimize the error. This is done using the chain rule of calculus.\n4. **Weight Update**: The weights and biases of each layer are updated based on the calculated error and the gradients of the loss function with respect to each weight and bias.\n5. **Repeat**: Steps 1-4 are repeated for multiple iterations, with the network adjusting its weights and biases to minimize the error.\n\n**Key Components of Backpropagation**\n\n* **Activation Functions**: Each layer uses an activation function to introduce non-linearity into the network. Common activation functions include sigmoid, ReLU (rectified linear unit), and tanh.\n* **Gradients**: The gradients of the loss function with respect to each weight and bias are calculated using the chain rule. These gradients are used to update the weights and biases.\n* **Optimizers**: Optimizers, such as stochastic gradient descent (SGD), Adam, or RMSProp, are used to update the weights and biases based on the calculated gradients.\n\n**Example**\n\nSuppose we have a simple neural network with two inputs, one hidden layer with two neurons, and one output. We want to train the network to predict the output of a simple function, such as `y = 2x + 1`.\n\n1. Forward pass: The network processes the input `x` and produces an output `y_pred`.\n2. Error calculation: The error between `y_pred` and the actual output `y` is calculated using MSE.\n3. Backward pass: The error is propagated backwards through the network, adjusting the weights and biases of each layer to minimize the error.\n4. Weight update: The weights and biases of each layer are updated based on the calculated error and the gradients of the loss function with respect to each weight and bias.\n\n**Conclusion**\n\nBackpropagation is a powerful algorithm for training neural networks. By iteratively adjusting the weights and biases of each layer to minimize the error, the network can learn to make accurate predictions on complex tasks. I hope this explanation helps! Do you have any specific questions about backpropagation or neural networks?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### User: Thanks! What about dropout in training?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"** Assistant:** We've already discussed dropout in training earlier. To recap, **dropout is a regularization technique used in neural networks to prevent overfitting**. It was introduced by Geoffrey Hinton and his colleagues in 2012.\n\n**What is Dropout?**\n\nDropout is a simple yet effective technique that randomly sets a fraction of the neurons in a layer to zero during training. This means that during each iteration, a random subset of neurons is temporarily removed from the network, and the remaining neurons are used to make predictions.\n\n**How Does Dropout Work?**\n\nHere's how dropout works:\n\n1. **Randomly Select Neurons**: During each iteration, a random subset of neurons is selected to be dropped out.\n2. **Set Neurons to Zero**: The selected neurons are set to zero, effectively removing them from the network.\n3. **Scale the Remaining Neurons**: The remaining neurons are scaled up to compensate for the removed neurons.\n4. **Make Predictions**: The network makes predictions using the remaining neurons.\n5. **Backpropagate**: The error is backpropagated through the network, adjusting the weights and biases of the remaining neurons.\n\n**Benefits of Dropout**\n\nDropout has several benefits:\n\n* **Prevents Overfitting**: By randomly removing neurons, dropout prevents the network from relying too heavily on any individual neuron or group of neurons.\n* **Encourages Ensemble Learning**: Dropout encourages the network to learn multiple representations of the data, effectively creating an ensemble of models.\n* **Improves Generalization**: Dropout helps the network generalize better to new, unseen data.\n\n**Hyperparameters for Dropout**\n\nThe key hyperparameter for dropout is the dropout rate, which controls the fraction of neurons to be dropped out. A common range for the dropout rate is between 0.2 and 0.5.\n\n**Example**\n\nSuppose we have a neural network with a hidden layer of 100 neurons, and we want to apply a dropout rate of 0.2. During each iteration, 20 neurons (0.2 x 100) would be randomly selected to be dropped out, and the remaining 80 neurons would be used to make predictions.\n\n**Code Example**\n\nIn Python, using the Keras library, you can implement dropout as follows:\n```python\nfrom keras.layers import Dropout\n\n# Create a neural network model\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))  # Apply dropout with a rate of 0.2\nmodel.add(Dense(10, activation='softmax'))\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n```\nIn this example, the `Dropout` layer is applied to the output of the first dense layer, with a dropout rate of 0.2.\n\nDo you have any specific questions about dropout or neural networks?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### User: Remind me what you said earlier about backpropagation."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"** Assistant:** Earlier, I explained how backpropagation works in neural networks. I described it as a fundamental concept in neural networks, and an algorithm used to train artificial neural networks. \n\nI broke down the backpropagation process into the following steps:\n\n1. **Forward Pass**: The network processes the input data and produces an output.\n2. **Error Calculation**: The error between the predicted output and the actual output is calculated using a loss function.\n3. **Backward Pass**: The error is propagated backwards through the network, adjusting the weights and biases of each layer to minimize the error.\n4. **Weight Update**: The weights and biases of each layer are updated based on the calculated error and the gradients of the loss function with respect to each weight and bias.\n5. **Repeat**: Steps 1-4 are repeated for multiple iterations, with the network adjusting its weights and biases to minimize the error.\n\nI also mentioned some key components of backpropagation, including:\n\n* **Activation Functions**: Each layer uses an activation function to introduce non-linearity into the network.\n* **Gradients**: The gradients of the loss function with respect to each weight and bias are calculated using the chain rule.\n* **Optimizers**: Optimizers, such as stochastic gradient descent (SGD), Adam, or RMSProp, are used to update the weights and biases based on the calculated gradients.\n\nAdditionally, I provided an example of how backpropagation works in a simple neural network, and concluded that backpropagation is a powerful algorithm for training neural networks, allowing them to learn and make accurate predictions on complex tasks. \n\nWould you like me to elaborate on any of these points or provide further clarification?"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"WJmGRihsalae","executionInfo":{"status":"ok","timestamp":1750173234208,"user_tz":-330,"elapsed":7,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":27,"outputs":[]}]}