# -*- coding: utf-8 -*-
"""ConversationBufferMemoryExample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C7hqNPWnEvnNhepDFgrCMur2Vnw3sDZR

## 2. `ConversationBufferMemory`
- **Purpose**: Keeps the **entire conversation history** verbatim in memory.
- **Use‚Äëcase**: Short chit‚Äëchat bots where full context is valuable.
- **Limitations**: May hit LLM context‚Äëwindow limits as conversation grows :contentReference[oaicite:1]{index=1}.
"""

# ================================
# Step 1: Install dependencies
# ================================
!pip install -q langchain langchain-groq

# ================================
# Step 2: Import libraries
# ================================
from langchain_groq import ChatGroq
from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableWithMessageHistory
from langchain.memory import ConversationBufferMemory
from IPython.display import Markdown, display
from google.colab import userdata

# ================================
# Step 3: Setup Groq API + LLM
# ================================
api_key = userdata.get("GROQ_API_KEY")  # or hardcode it: "sk-..."
llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    api_key=api_key,
    temperature=0.3,
)

# ================================
# Step 4: Define Prompt Template
# ================================
# The system message sets the tone
system_msg = SystemMessagePromptTemplate.from_template(
    "You are a friendly, knowledgeable assistant. Answer user queries clearly and remember previous interactions."
)

# The MessagesPlaceholder allows memory to inject chat history dynamically
human_msg = HumanMessagePromptTemplate.from_template("{input}")

chat_prompt = ChatPromptTemplate.from_messages([
    system_msg,
    MessagesPlaceholder(variable_name="history"),  # enables memory tracking
    human_msg
])

# ================================
# Step 5: Setup ConversationBufferMemory
# ================================
# ConversationBufferMemory stores all user/assistant messages as-is
# Best for short conversations where token cost isn't a concern

memory = ConversationBufferMemory(return_messages=True)

# ================================
# Step 6: Combine LLM, Prompt, and Memory
# ================================
chat_chain = chat_prompt | llm

# Wrap the chain with memory support
chatbot = RunnableWithMessageHistory(
    chat_chain,
    lambda session_id: memory.chat_memory,
    input_messages_key="input",
    history_messages_key="history"
)

# ================================
# ü§ñ Step 7: Start the Chat Loop
# ================================
session_id = "chat-session-001"

user_inputs = [
    "Hi! What is a confusion matrix?",
    "Can you explain how it's used in model evaluation?",
    "Thanks! What are its limitations?"
]

for input_text in user_inputs:
    response = chatbot.invoke(
        {"input": input_text},
        config={"configurable": {"session_id": session_id}}
    )

    display(Markdown(f"### ‚ùì User: {input_text}"))
    display(Markdown(f"**ü§ñ Assistant:** {response.content}"))

