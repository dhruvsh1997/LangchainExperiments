# -*- coding: utf-8 -*-
"""ChatMessagePromptTemplateExample-RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r4c-s9K2kePMvmfxDlWnU_iGg68TzG_H

## 3. `ChatMessagePromptTemplate`

### Description:
- This is a lower-level component that defines a **single message** in a chat sequence.
- Can be of type: `SystemMessagePromptTemplate`, `HumanMessagePromptTemplate`, `AIMessagePromptTemplate`, etc.
- These are building blocks used inside a `ChatPromptTemplate`.

### Key Use-Cases:
- Fine-grained control over message types in multi-turn chat prompts.
- When you want to mix system instructions with user and assistant inputs explicitly.
- Customizing personality, tone, or task instructions per message role.
"""

# ===================== INSTALL DEPENDENCIES =====================
!pip install -q langchain sentence-transformers faiss-cpu pypdf groq langchain-community langchain-groq

# ===================== IMPORTS =====================
import os
import torch
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatMessagePromptTemplate
)
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from sentence_transformers.cross_encoder import CrossEncoder
from IPython.display import display, Markdown

# ===================== LOAD & SPLIT PDF =====================
loader = PyPDFLoader("/content/solid-python.pdf")
documents = loader.load_and_split()

splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)
print(f"‚úÖ Total Chunks Created: {len(docs)}")

# ===================== EMBEDDINGS + FAISS =====================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embedding_model)

# ===================== üîç RETRIEVER WITH CUSTOM MMR =====================
retriever = vectorstore.as_retriever(
    search_type="mmr", search_kwargs={"k": 15, "fetch_k": 30}, lambda_mult=0.3
)

# ===================== DEFINE LLM =====================
from google.colab import userdata
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    api_key=userdata.get('GROQ_API_KEY')  # Replace with your Groq API key
)

# ===================== üí¨ CHAT PROMPT USING ChatMessagePromptTemplate =====================
# Define the full message sequence using ChatMessagePromptTemplate
system_message = SystemMessagePromptTemplate.from_template(
    "You are a knowledgeable assistant helping users answer questions using the given context."
)
human_message = HumanMessagePromptTemplate.from_template(
    "Here is the context:\n\n{context}\n\nQuestion: {question}"
)

# ===================== RERANKER INITIALIZATION =====================
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L6-v2")

# Combine into ChatPromptTemplate
chat_prompt = ChatPromptTemplate.from_messages([
    ChatMessagePromptTemplate(role="system", prompt=system_message.prompt),
    ChatMessagePromptTemplate(role="user", prompt=human_message.prompt)
])

question = "What is the main objective of the document?"
retrieved_docs = retriever.get_relevant_documents(question)

# Answer before reranking (top 3 chunks)
context_before = "\n\n".join([doc.page_content for doc in retrieved_docs[:3]])
messages_before = chat_prompt.format_messages(context=context_before, question=question)
answer_before = llm.invoke(messages_before)

display(Markdown("### Final Answer (Before Reranking):"))
display(Markdown(answer_before.content))

# ===================== DISPLAY TOP-K (BEFORE RERANKING) =====================
# print("\nüîπ Top K Retrieved Chunks (Before Reranking):")
# for i, doc in enumerate(retrieved_docs):
#     page = doc.metadata.get("page", "Unknown")
#     print(f"\n--- Chunk {i+1} ---")
#     print(f"Page: {page}")
#     print(f"Content:\n{doc.page_content[:10]}...")

# ===================== RERANKING =====================
pairs = [[question, doc.page_content] for doc in retrieved_docs]
scores = reranker.predict(pairs)
scored_docs = list(zip(retrieved_docs, scores))
sorted_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)

# ===================== üî∏ DISPLAY TOP-K (AFTER RERANKING) =====================
print("\nüî∏ Reranked Chunks (CrossEncoder):")
for i, (doc, score) in enumerate(sorted_docs[:5]):
    page = doc.metadata.get("page", "Unknown")
    print(f"\n--- Reranked Chunk {i+1} ---")
    print(f"Page: {page}")
    print(f"Score: {score:.4f}")
    print(f"Content:\n{doc.page_content[:30]}...")

# Answer after reranking (top 3 chunks)
top_reranked_docs = [doc for doc, _ in sorted_docs[:3]]
context_after = "\n\n".join([doc.page_content for doc in top_reranked_docs])
messages_after = chat_prompt.format_messages(context=context_after, question=question)
answer_after = llm.invoke(messages_after)

display(Markdown("### Final Answer (Before Reranking):"))
display(Markdown(answer_before.content))

display(Markdown("### Final Answer (After Reranking):"))
display(Markdown(answer_after.content))







"""## 4. `ImagePromptTemplate`

### Description:
- Specialized template for **multimodal prompting**, particularly when the model can accept images (e.g., GPT-4o, Gemini, etc.).
- Defines how image inputs should be passed alongside text to the model.

### Key Use-Cases:
- When using models that support both **image and text input**.
- RAG pipelines involving visual documents (e.g., scanned PDFs, screenshots).
- Vision-Language tasks such as OCR + Q&A, diagram interpretation, etc.

---

## 5. `PipelinePromptTemplate`

### Description:
- A **composable template** for chaining multiple `PromptTemplates` together.
- Allows you to pass the output of one prompt as input to another.
- Supports multi-stage reasoning or multi-step prompt workflows.

### Key Use-Cases:
- Advanced RAG pipelines where different stages of retrieval, summarization, reformulation, or reasoning need distinct prompt phases.
- Tasks like: initial retrieval ‚Üí reformulate query ‚Üí generate answer.
- Enables modular design of prompt logic for agentic workflows.

---

## Summary of Differences

| Prompt Type               | Input Format          | Target Models        | Use-case Complexity | Usage Context                    |
|---------------------------|------------------------|----------------------|---------------------|----------------------------------|
| `PromptTemplate`          | Plain text             | Text LLMs            | Basic               | Simple prompts and QA            |
| `ChatPromptTemplate`      | Structured messages    | Chat LLMs            | Medium to High      | Conversational AI, agents        |
| `ChatMessagePromptTemplate`| One chat message      | Chat LLMs            | Modular             | Fine-tuning chat interaction     |
| `ImagePromptTemplate`     | Text + Image           | Multimodal LLMs      | Specialized         | Image + Text based RAGs          |
| `PipelinePromptTemplate`  | Multi-step templates   | Any (chained flows)  | Advanced            | Complex RAG or multi-step logic  |

now help me create a colab notebook code in an interprative format with proper code, the flow will be loading a static PDF located in current directory called "Sample.pdf" load it through PyPDFLoader, then chunk it using CharacterTextSplitter, then use sentence transformer's all-MiniLM-L6-V2 embedding model, then use FAISS Vactorstore for storing vectors, then as_retriever(search_type="mmr"), then ChatGroq with 'llama-3.3-70b-versatile' as the main LLM model, then with PromptTemplate for providing the prompt. and then perform question answering with properly showing all the top k selected chunks(which page those chunks belong to), respective similarity score, also must use reranker cross-encoder/ms-marco-MiniLM-L6-v2 and show the same thing for that, and the final answer before and after reranker.
"""

