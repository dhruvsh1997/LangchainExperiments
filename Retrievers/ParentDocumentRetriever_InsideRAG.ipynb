{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UWHhVWmzmwY",
        "outputId": "2ed30d5b-e88a-4aa5-87d2-d1de044ec442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.6/304.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ===================== INSTALL DEPENDENCIES =====================\n",
        "!pip install -q langchain sentence-transformers faiss-cpu pypdf langchain-community langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== IMPORTS =====================\n",
        "import os\n",
        "import torch\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from IPython.display import display, Markdown\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gaWg3T709uCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== LOAD PDF =====================\n",
        "loader = PyPDFLoader(\"/content/solid-python.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "# Assign parent IDs\n",
        "for i, doc in enumerate(documents):\n",
        "    doc.metadata[\"doc_id\"] = f\"doc_{i}\""
      ],
      "metadata": {
        "id": "jkxkVtc39CA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== SPLITTING =====================\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "text_splitter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FtTEdDT9yyi",
        "outputId": "00e115b0-e782-4b4a-95b0-268e1e431418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_text_splitters.character.CharacterTextSplitter at 0x7a5c67d73050>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== EMBEDDINGS =====================\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedding_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CpKcJIp94fa",
        "outputId": "a7795831-ce4a-447e-d08f-ca7aa9d52788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "  (2): Normalize()\n",
              "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== VECTOR STORE + DOC STORE =====================\n",
        "# Create a FAISS vector store from the documents and embeddings\n",
        "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
        "docstore = InMemoryStore()  # Stores parent docs"
      ],
      "metadata": {
        "id": "lm1ujI0494cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================  RETRIEVER WITH CUSTOM MMR =====================\n",
        "# retriever = vectorstore.as_retriever(\n",
        "#     search_type=\"mmr\", search_kwargs={\"k\": 15, \"fetch_k\": 30}, lambda_mult=0.3\n",
        "# )"
      ],
      "metadata": {
        "id": "ev87flzl_Eoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieves parent document from child chunks (preserves context), preffered on Long documents with semantic hierarchy, Needs ParentDocumentRetriever setup"
      ],
      "metadata": {
        "id": "gKuKP-9OBRAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== PARENT RETRIEVER WITH MMR =====================\n",
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=docstore,\n",
        "    child_splitter=text_splitter,\n",
        "    parent_splitter=None,  # no parent split, original docs are used\n",
        ")\n",
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPCXE3Eb94Z7",
        "outputId": "fba2d001-1b95-4ac7-a325-2d60b317fea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParentDocumentRetriever(vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7a5c6b47ba10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7a5c6b23dc90>, search_kwargs={}, child_splitter=<langchain_text_splitters.character.CharacterTextSplitter object at 0x7a5c67d73050>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== ADD DOCUMENTS =====================\n",
        "retriever.add_documents(documents)"
      ],
      "metadata": {
        "id": "GJ9MIH6T94Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== LLM SETUP =====================\n",
        "from google.colab import userdata\n",
        "llm = ChatGroq(\n",
        "    model_name=\"llama-3.3-70b-versatile\",\n",
        "    api_key=userdata.get(\"GROQ_API_KEY\")\n",
        ")\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6SJmc9b94UV",
        "outputId": "fe797aee-76e8-40ec-d6e1-d3113aa04d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7a5c67721790>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7a5c6771fd50>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== PROMPT =====================\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
        ")"
      ],
      "metadata": {
        "id": "7XS8ExW4_UOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== BUILD RAG CHAIN =====================\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": prompt_template}\n",
        ")\n",
        "qa_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "725pBeBh_Wzj",
        "outputId": "d6b98c5b-5c18-4999-fea0-cf5994711c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RetrievalQA(verbose=False, combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {question}'), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7a5c67721790>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7a5c6771fd50>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), retriever=ParentDocumentRetriever(vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7a5c6b47ba10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7a5c6b23dc90>, search_kwargs={}, child_splitter=<langchain_text_splitters.character.CharacterTextSplitter object at 0x7a5c67d73050>))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== RUN A QUERY =====================\n",
        "question = \"What is the main objective of the document?\"\n",
        "result = qa_chain.invoke({\"query\": question})\n",
        "print(\" Answer:\", result[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S75DsAbR_ZSn",
        "outputId": "0b1d0988-0ed1-404b-82c4-ac42eadf17bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Answer: The main objective of the document is to discuss the principles of SOLID software design, specifically the 5 aspects of a class and the corresponding 5 principles (SOLID) for software design, as presented by Mike Lindner.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== METRIC EVALUATION HELPERS =====================\n",
        "\n",
        "def precision_recall_f1(retrieved_ids, relevant_ids):\n",
        "    intersection = set(retrieved_ids).intersection(set(relevant_ids))\n",
        "    precision = len(intersection) / len(retrieved_ids) if retrieved_ids else 0\n",
        "    recall = len(intersection) / len(relevant_ids) if relevant_ids else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8) if (precision + recall) > 0 else 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "def mean_reciprocal_rank(retrieved_ids, relevant_ids):\n",
        "    for i, doc_id in enumerate(retrieved_ids):\n",
        "        if doc_id in relevant_ids:\n",
        "            return 1 / (i + 1)\n",
        "    return 0.0\n",
        "\n",
        "def hit_at_k(retrieved_ids, relevant_ids, k):\n",
        "    return int(any(doc_id in retrieved_ids[:k] for doc_id in relevant_ids))\n",
        "\n",
        "def faithfulness_check(llm, answer, context_docs):\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in context_docs[:5]])\n",
        "    prompt = f\"Context:\\n{context}\\n\\nAnswer:\\n{answer}\\n\\nIs the answer supported by the context? Answer 'yes' or 'no'.\"\n",
        "    verdict = llm.invoke(prompt)\n",
        "    return 1 if \"yes\" in verdict.content.lower() else 0"
      ],
      "metadata": {
        "id": "j3MGSmCM_a8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== QUERY SETUP =====================\n",
        "queries = [\n",
        "    \"What are the five solid principles?\",\n",
        "    \"Why is dependency inversion important?\",\n",
        "    \"What is the difference between interface segregation and Liskov substitution?\",\n",
        "]\n",
        "\n",
        "ground_truths = [\n",
        "    [\"doc_3\", \"doc_7\"],     # Expected relevant docs for query 1\n",
        "    [\"doc_5\"],              # Expected relevant docs for query 2\n",
        "    [\"doc_6\", \"doc_8\"],     # Expected relevant docs for query 3\n",
        "]"
      ],
      "metadata": {
        "id": "XiPhbYX__gGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== RUN EVALUATION =====================\n",
        "all_precisions, all_recalls, all_f1s, all_mrrs, all_hits, all_faith = [], [], [], [], [], []\n",
        "\n",
        "for i, question in enumerate(queries):\n",
        "    print(f\"\\n Query: {question}\")\n",
        "\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "    retrieved_ids = [doc.metadata.get(\"doc_id\") for doc in retrieved_docs]\n",
        "\n",
        "    precision, recall, f1 = precision_recall_f1(retrieved_ids, ground_truths[i])\n",
        "    mrr = mean_reciprocal_rank(retrieved_ids, ground_truths[i])\n",
        "    hit = hit_at_k(retrieved_ids, ground_truths[i], k=5)\n",
        "\n",
        "    answer = qa_chain.invoke({\"query\": question})\n",
        "    is_faithful = faithfulness_check(llm, answer[\"result\"], retrieved_docs)\n",
        "\n",
        "    all_precisions.append(precision)\n",
        "    all_recalls.append(recall)\n",
        "    all_f1s.append(f1)\n",
        "    all_mrrs.append(mrr)\n",
        "    all_hits.append(hit)\n",
        "    all_faith.append(is_faithful)\n",
        "\n",
        "    print(f\"Answer: {answer['result']}\")\n",
        "    print(f\"Precision: {precision:.2f} | Recall: {recall:.2f} | F1: {f1:.2f} | MRR: {mrr:.2f} | Hit@5: {hit} | Faithful: {is_faithful}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M5AsrGh_jfK",
        "outputId": "d8b34faf-5eac-4190-95f7-54e427c8216b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Query: What are the five solid principles?\n",
            "Answer: The five SOLID principles are:\n",
            "\n",
            "1. Single Responsibility Principle\n",
            "2. Open-Closed Principle\n",
            "3. Liskov Substitution Principle\n",
            "4. Interface Segregation Principle\n",
            "5. Dependency Inversion Principle\n",
            "\n",
            "These principles are guidelines for designing solid, maintainable, and scalable software systems.\n",
            "Precision: 0.00 | Recall: 0.00 | F1: 0.00 | MRR: 0.00 | Hit@5: 0 | Faithful: 1\n",
            "\n",
            " Query: Why is dependency inversion important?\n",
            "Answer: The context provided doesn't explicitly state why dependency inversion is important. However, based on general knowledge of the SOLID principles, dependency inversion is important because it helps to reduce coupling between classes and makes the system more modular, flexible, and easier to test.\n",
            "\n",
            "Dependency inversion principle states that high-level modules should not depend on low-level modules, but both should depend on abstractions. This means that instead of a high-level module depending directly on a low-level module, both modules depend on an interface or abstraction. This helps to decouple the modules and makes it easier to change or replace one module without affecting the other.\n",
            "\n",
            "By following the dependency inversion principle, developers can create systems that are more maintainable, scalable, and robust. It also helps to reduce the risk of cascading changes, where a change to one module affects multiple other modules.\n",
            "Precision: 0.50 | Recall: 1.00 | F1: 0.67 | MRR: 1.00 | Hit@5: 1 | Faithful: 0\n",
            "\n",
            " Query: What is the difference between interface segregation and Liskov substitution?\n",
            "Answer: The difference between Interface Segregation and Liskov Substitution lies in their focus and application:\n",
            "\n",
            "1. **Interface Segregation Principle (ISP)**: This principle states that a client should not be forced to depend on interfaces it does not use. It focuses on the design of interfaces, emphasizing that they should be split into smaller, more specialized interfaces that meet the specific needs of clients, rather than having a large, fat interface that contains methods that might not be relevant to all clients. The goal is to reduce coupling and make the system more modular and maintainable.\n",
            "\n",
            "2. **Liskov Substitution Principle (LSP)**: This principle is concerned with the relationship between subtypes and their supertypes, stating that subtypes should be substitutable for their supertypes. It focuses on ensuring that any code that uses a supertype can work with a subtype without knowing the difference. The LSP is about maintaining the correctness of a program by ensuring that subtypes do not alter the behavior of their supertypes in ways that would break the expectations of code using the supertype.\n",
            "\n",
            "In summary, Interface Segregation is about designing interfaces to be client-centric and minimalistic to avoid unnecessary dependencies, while Liskov Substitution is about ensuring that inheritance relationships are designed in a way that subtypes can replace their supertypes without affecting the correctness of the program. Both principles aim to improve the maintainability, flexibility, and scalability of software systems but address different aspects of software design.\n",
            "Precision: 0.00 | Recall: 0.00 | F1: 0.00 | MRR: 0.00 | Hit@5: 0 | Faithful: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== METRIC SUMMARY =====================\n",
        "print(\"\\n Average Metrics Across Queries\")\n",
        "print(f\"Precision@10: {np.mean(all_precisions):.2f}\") #Of the top k retrieved documents, how many are actually relevant?\n",
        "print(f\"Recall@10: {np.mean(all_recalls):.2f}\")#How many of the total relevant documents were retrieved?\n",
        "print(f\"F1 Score: {np.mean(all_f1s):.2f}\")#Harmonic Mean of Ps/Rc - (# of relevant documents retrieved) / (Total relevant docs)\n",
        "print(f\"MRR: {np.mean(all_mrrs):.2f}\") #Maximal Marginal Relevance - Helps balance between relevance and diversity in results.\n",
        "print(f\"Hit@5: {np.mean(all_hits):.2f}\") #mAP (Mean Average Precision)\n",
        "print(f\"Faithfulness: {np.mean(all_faith):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksZ46kOx_lra",
        "outputId": "b9742d86-8d85-4d2c-ff66-89ea3eef1144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Average Metrics Across Queries\n",
            "Precision: 0.17\n",
            "Recall: 0.33\n",
            "F1 Score: 0.22\n",
            "MRR: 0.33\n",
            "Hit@5: 0.33\n",
            "Faithfulness: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqLetKsG_vXN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}