# -*- coding: utf-8 -*-
"""VectorStoreRetrieverMMR-InsideRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QLR60qxmKZS6D0WgU-n5Qt9MFPCJoYDF
"""

# ===================== INSTALL DEPENDENCIES =====================
!pip install -q langchain sentence-transformers faiss-cpu pypdf langchain-community langchain-groq

# ===================== IMPORTS =====================
import os
import torch
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_groq import ChatGroq
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore

from sentence_transformers.cross_encoder import CrossEncoder
from IPython.display import display, Markdown
import numpy as np

# ===================== LOAD PDF =====================
loader = PyPDFLoader("/content/solid-python.pdf")
documents = loader.load()

# Assign parent IDs
for i, doc in enumerate(documents):
    doc.metadata["doc_id"] = f"doc_{i}"

# ===================== SPLITTING =====================
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
text_splitter

# ===================== EMBEDDINGS =====================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
embedding_model

# ===================== VECTOR STORE + DOC STORE =====================
# Create a FAISS vector store from the documents and embeddings
vectorstore = FAISS.from_documents(documents, embedding_model)
docstore = InMemoryStore()  # Stores parent docs

"""Basic kNN/Similarity-based retrieval from embeddings, preffered on Small to medium documents, can be used on Any Vector DB"""

# =====================  RETRIEVER WITH CUSTOM MMR =====================
retriever = vectorstore.as_retriever(
    search_type="mmr", search_kwargs={"k": 15, "fetch_k": 30}, lambda_mult=0.3
)

"""###MMR (Maximal Marginal Relevance) is a ranking algorithm used in retrieval systems to balance:
1. Relevance (how similar the document is to the query)
2. Diversity (how different it is from already selected documents)
####lambda_mult
** λ Value	Behavior <br/>
** λ = 1.0	Only relevance (normal similarity search)<br/>
** λ = 0.0	Only diversity (may miss relevance)<br/>
** λ = 0.3–0.7	Balanced mix (recommended range)<br/>
<br/>
-- scalable retrieval using Approximate Nearest Neighbor (ANN)
"""

# ===================== ADD DOCUMENTS =====================
retriever.add_documents(documents)

# ===================== LLM SETUP =====================
from google.colab import userdata
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    api_key=userdata.get("GROQ_API_KEY")
)
llm

# ===================== PROMPT =====================
prompt_template = PromptTemplate.from_template(
    "Use the following context to answer the question:\n\n{context}\n\nQuestion: {question}"
)

# ===================== BUILD RAG CHAIN =====================
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt_template}
)
qa_chain

# ===================== RUN A QUERY =====================
question = "What is the main objective of the document?"
result = qa_chain.invoke({"query": question})
print(" Answer:", result["result"])

# ===================== METRIC EVALUATION HELPERS =====================
def precision_recall_f1(retrieved_ids, relevant_ids):
    intersection = set(retrieved_ids).intersection(set(relevant_ids))
    precision = len(intersection) / len(retrieved_ids) if retrieved_ids else 0
    recall = len(intersection) / len(relevant_ids) if relevant_ids else 0
    f1 = 2 * precision * recall / (precision + recall + 1e-8) if (precision + recall) > 0 else 0
    return precision, recall, f1

def mean_reciprocal_rank(retrieved_ids, relevant_ids):
    for i, doc_id in enumerate(retrieved_ids):
        if doc_id in relevant_ids:
            return 1 / (i + 1)
    return 0.0

def hit_at_k(retrieved_ids, relevant_ids, k):
    return int(any(doc_id in retrieved_ids[:k] for doc_id in relevant_ids))

def faithfulness_check(llm, answer, context_docs):
    context = "\n\n".join([doc.page_content for doc in context_docs[:5]])
    prompt = f"Context:\n{context}\n\nAnswer:\n{answer}\n\nIs the answer supported by the context? Answer 'yes' or 'no'."
    verdict = llm.invoke(prompt)
    return 1 if "yes" in verdict.content.lower() else 0

# ===================== QUERY SETUP =====================
queries = [
    "What are the five solid principles?",
    "Why is dependency inversion important?",
    "What is the difference between interface segregation and Liskov substitution?",
]

ground_truths = [
    ["doc_3", "doc_7"],     # Expected relevant docs for query 1
    ["doc_5"],              # Expected relevant docs for query 2
    ["doc_6", "doc_8"],     # Expected relevant docs for query 3
]

# ===================== RUN EVALUATION =====================
all_precisions, all_recalls, all_f1s, all_mrrs, all_hits, all_faith = [], [], [], [], [], []

for i, question in enumerate(queries):
    print(f"\n Query: {question}")

    retrieved_docs = retriever.get_relevant_documents(question)
    retrieved_ids = [doc.metadata.get("doc_id") for doc in retrieved_docs]

    precision, recall, f1 = precision_recall_f1(retrieved_ids, ground_truths[i])
    mrr = mean_reciprocal_rank(retrieved_ids, ground_truths[i])
    hit = hit_at_k(retrieved_ids, ground_truths[i], k=5)

    answer = qa_chain.invoke({"query": question})
    is_faithful = faithfulness_check(llm, answer["result"], retrieved_docs)

    all_precisions.append(precision)
    all_recalls.append(recall)
    all_f1s.append(f1)
    all_mrrs.append(mrr)
    all_hits.append(hit)
    all_faith.append(is_faithful)

    print(f"Answer: {answer['result']}")
    print(f"Precision: {precision:.2f} | Recall: {recall:.2f} | F1: {f1:.2f} | MRR: {mrr:.2f} | Hit@5: {hit} | Faithful: {is_faithful}")

# ===================== METRIC SUMMARY =====================
print("\n Average Metrics Across Queries")
print(f"Precision: {np.mean(all_precisions):.2f}")
print(f"Recall: {np.mean(all_recalls):.2f}")
print(f"F1 Score: {np.mean(all_f1s):.2f}")
print(f"MRR: {np.mean(all_mrrs):.2f}")
print(f"Hit@5: {np.mean(all_hits):.2f}")
print(f"Faithfulness: {np.mean(all_faith):.2f}")

