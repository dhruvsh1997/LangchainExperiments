{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# +------------------------+\n",
        "# |  Input Prompt          |\n",
        "# +------------------------+\n",
        "#          |\n",
        "#          v\n",
        "# +------------------------+\n",
        "# |         LLM            |\n",
        "# +------------------------+\n",
        "#          |\n",
        "#          v\n",
        "# +------------------------+\n",
        "# | StructuredOutputParser |\n",
        "# |  (validates JSON)      |\n",
        "# +------------------------+\n",
        "#          |\n",
        "#          v\n",
        "# +------------------------+\n",
        "# | Dict: {answer, source} |\n",
        "# +------------------------+"
      ],
      "metadata": {
        "id": "F97NBo53sgl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h7sWtslrFrC",
        "outputId": "3db04f35-5f51-46bc-9afa-cbe8faa178af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# STEP 1: Install LangChain, Groq integration, and Pydantic.\n",
        "!pip install -q langchain langchain-groq langchain_community pydantic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Gather your spellbooks!\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder\n",
        ")\n",
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import (\n",
        "    BaseChatMessageHistory,\n",
        "    InMemoryChatMessageHistory\n",
        ")\n",
        "from langchain.output_parsers.structured import StructuredOutputParser\n",
        "from langchain.output_parsers import ResponseSchema\n",
        "from pydantic import BaseModel, Field\n",
        "from IPython.display import Markdown, display\n",
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "6Z44E0u_rVN8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Whisper your secret key to the winds.\n",
        "try:\n",
        "    api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "except Exception:\n",
        "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"GROQ_API_KEY not found. Please set it!\")\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    api_key=api_key,\n",
        "    temperature=0.3,\n",
        ")"
      ],
      "metadata": {
        "id": "uHq0lpMSrZhu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Define the fields you want the LLM to produce.\n",
        "# ----------------------------------------------------------\n",
        "# The StructuredOutputParser uses ResponseSchema definitions\n",
        "# to generate a validated output.\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "response_schemas = [\n",
        "    ResponseSchema(\n",
        "        name=\"answer\",\n",
        "        description=\"A clear explanation of the concept asked about.\"\n",
        "    ),\n",
        "    ResponseSchema(\n",
        "        name=\"source\",\n",
        "        description=\"Where the information came from, like a reference or URL.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create the parser with your schema\n",
        "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "# Get format instructions you must embed in your prompt\n",
        "format_instructions = parser.get_format_instructions()"
      ],
      "metadata": {
        "id": "1MBfWXdmrgbD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Weave the format instructions into your system message.\n",
        "# ---------------------------------------------------------------\n",
        "# Use {format_instructions} as a placeholder so you can supply them safely.\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "system_msg = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Please format your response to match these instructions:\\n{format_instructions}\"\n",
        ")\n",
        "\n",
        "human_msg = HumanMessagePromptTemplate.from_template(\n",
        "    \"{input}\"\n",
        ")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_msg,\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    human_msg\n",
        "])\n",
        "\n",
        "# Partial: provide the format instructions to fill in the placeholder\n",
        "chat_prompt = chat_prompt.partial(format_instructions=format_instructions)"
      ],
      "metadata": {
        "id": "gJn5xeQ_rgXK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Keep your conversation history.\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]"
      ],
      "metadata": {
        "id": "9bihARzcrgTT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Chain your LLM + parser + memory.\n",
        "# ---------------------------------------------------------------\n",
        "# ⚡ The StructuredOutputParser ensures the output is validated\n",
        "# and parsed as a dict.\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "chat_chain = chat_prompt | llm | parser\n",
        "\n",
        "# Wrap with memory support\n",
        "chatbot = RunnableWithMessageHistory(\n",
        "    chat_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "vj3YIM_-rgPu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Test it!\n",
        "session_id = \"chat-session-structured-001\"\n",
        "\n",
        "user_inputs = [\n",
        "    \"Explain what a confusion matrix is with a source.\",\n",
        "    \"Now explain what precision and recall mean, and give a source.\"\n",
        "]\n",
        "\n",
        "print(f\"Starting chat loop with session ID: {session_id}\")\n",
        "\n",
        "for input_text in user_inputs:\n",
        "    print(f\"\\nUser: {input_text}\")\n",
        "    response = chatbot.invoke(\n",
        "        {\"input\": input_text},\n",
        "        config={\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    # The StructuredOutputParser returns a dict with validated fields\n",
        "    display(Markdown(f\"**Answer:** {response['answer']}\\n\\n**Source:** {response['source']}\"))\n",
        "\n",
        "print(\"\\n--- Stored Chat History ---\")\n",
        "for message in store[session_id].messages:\n",
        "    print(f\"{message.type.capitalize()}: {message.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "Y62-O28_rgL2",
        "outputId": "f8076bd1-40eb-42f9-9202-72bde99bbd26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chat loop with session ID: chat-session-structured-001\n",
            "\n",
            "User: Explain what a confusion matrix is with a source.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:** A confusion matrix is a table used to describe the performance of a classification model, such as a logistic regression or decision tree, on a set of test data for which the true values are known. The matrix itself is relatively simple to understand, but the related terminology can be confusing. The matrix has the following structure: True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).\n\n**Source:** https://en.wikipedia.org/wiki/Confusion_matrix"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: Now explain what precision and recall mean, and give a source.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:** Precision and recall are two fundamental metrics used to evaluate the performance of classification models in machine learning and information retrieval. Precision is the ratio of true positives (correctly predicted instances) to the sum of true positives and false positives (incorrectly predicted instances). It measures the accuracy of the model's positive predictions. Recall, on the other hand, is the ratio of true positives to the sum of true positives and false negatives (missed instances). It measures the model's ability to detect all instances of a particular class. A balance between precision and recall is often sought, as improving one can come at the expense of the other.\n\n**Source:** https://en.wikipedia.org/wiki/Precision_and_recall"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Stored Chat History ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2xDtvBpsWJ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}