{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH00Enuo9HOE",
        "outputId": "503f4585-fabb-4ad8-c931-9300c9d22d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# STEP 1: Install all the necessary libraries.\n",
        "# LangChain for orchestration, LangChain-Groq for Groq LLM.\n",
        "!pip install -q langchain langchain-groq langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers.json import JsonOutputParser"
      ],
      "metadata": {
        "id": "8u40D2EA-gko"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Gather all the wands and scrolls we shall need.\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory # <--- CORRECTED IMPORT\n",
        "from IPython.display import Markdown, display\n",
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "USyLZ7J19KdW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Whisper your secret key to the winds.\n",
        "try:\n",
        "    api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "except Exception:\n",
        "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"GROQ_API_KEY not found. Please set it!\")\n",
        "\n",
        "# STEP 3b: Summon the Groq-powered LLM.\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    api_key=api_key,\n",
        "    temperature=0.3,\n",
        ")"
      ],
      "metadata": {
        "id": "W7OGJ4Qt-mE4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Forge a prompt that commands your LLM to obey the JSON contract.\n",
        "# -------------------------------------------\n",
        "# 💡 IMPORTANT:\n",
        "# For JSONOutputParser to work, your prompt must tell the LLM to respond in JSON.\n",
        "# Otherwise the parser will fail — because it expects valid JSON.\n",
        "# -------------------------------------------\n",
        "\n",
        "system_msg = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a structured data assistant. Always reply in valid JSON with keys: 'answer' and 'source'.\"\n",
        ")\n",
        "\n",
        "human_msg = HumanMessagePromptTemplate.from_template(\n",
        "    \"{input}\"\n",
        ")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_msg,\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    human_msg\n",
        "])"
      ],
      "metadata": {
        "id": "M-R0USD4-ptk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: As before, keep your memories safe.\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n"
      ],
      "metadata": {
        "id": "8WOuoGe3-sON"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: 🌟 The heart of the magic.\n",
        "# -------------------------------------------\n",
        "# 💡 WHAT JsonOutputParser DOES:\n",
        "# It expects the raw LLM output to be a valid JSON string.\n",
        "# Then, it parses it into a Python dict.\n",
        "# If the LLM outputs invalid JSON, the parser will raise an error — so your prompt must guide it carefully.\n",
        "# -------------------------------------------\n",
        "\n",
        "chat_chain = chat_prompt | llm\n",
        "\n",
        "# Add the JsonOutputParser — it will parse text into a Python dict.\n",
        "parsed_chain = chat_chain | JsonOutputParser()\n",
        "\n",
        "# Wrap with memory support, so the chain remembers your tale.\n",
        "chatbot = RunnableWithMessageHistory(\n",
        "    parsed_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "ca6OqbP6-tqf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Test your chain.\n",
        "# You should see neat Python dictionaries coming back.\n",
        "\n",
        "session_id = \"chat-session-json-001\"\n",
        "\n",
        "user_inputs = [\n",
        "    \"Explain what is LangChain, cite the official docs URL as source.\",\n",
        "    \"What is JSONOutputParser used for? Cite the docs URL.\"\n",
        "]\n",
        "\n",
        "print(f\"Starting chat loop with session ID: {session_id}\")\n",
        "\n",
        "for input_text in user_inputs:\n",
        "    print(f\"\\nUser: {input_text}\")\n",
        "    response = chatbot.invoke(\n",
        "        {\"input\": input_text},\n",
        "        config={\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    # The response will be a Python dict: {'answer': ..., 'source': ...}\n",
        "    print(\"Parsed Response:\", response)\n",
        "    display(Markdown(f\"**Answer:** {response['answer']}\\n\\n**Source:** {response['source']}\"))\n",
        "\n",
        "print(\"\\n--- Stored Chat History ---\")\n",
        "for message in store[session_id].messages:\n",
        "    print(f\"{message.type.capitalize()}: {message.content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "IYgrMzmH-vCX",
        "outputId": "e07e27d9-6cf4-46f9-9c26-771228a9106d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chat loop with session ID: chat-session-json-001\n",
            "\n",
            "User: Explain what is LangChain, cite the official docs URL as source.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed Response: {'answer': 'LangChain is an open-source framework designed to help developers build applications powered by large language models. It provides a set of tools and libraries to simplify the process of integrating language models into various applications, allowing developers to focus on building their products rather than spending time on the underlying infrastructure.', 'source': 'https://langchain.readthedocs.io/en/latest/'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:** LangChain is an open-source framework designed to help developers build applications powered by large language models. It provides a set of tools and libraries to simplify the process of integrating language models into various applications, allowing developers to focus on building their products rather than spending time on the underlying infrastructure.\n\n**Source:** https://langchain.readthedocs.io/en/latest/"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: What is JSONOutputParser used for? Cite the docs URL.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: KeyError('output')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed Response: {'answer': 'JSONOutputParser is used for parsing JSON output from various sources, but the specific use case may vary depending on the context. Generally, it is utilized to extract and process data from JSON-formatted strings or files.', 'source': 'https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/regex/Pattern.html'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:** JSONOutputParser is used for parsing JSON output from various sources, but the specific use case may vary depending on the context. Generally, it is utilized to extract and process data from JSON-formatted strings or files.\n\n**Source:** https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/regex/Pattern.html"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Stored Chat History ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CdfZkuk8-zYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}